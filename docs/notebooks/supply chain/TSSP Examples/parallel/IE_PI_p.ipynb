{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:37:05.426617Z",
     "start_time": "2024-11-05T16:37:05.415333Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyomo.environ import *\n",
    "import mpisppy.utils.sputils as sputils\n",
    "from mpisppy.opt.ef import ExtensiveForm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import sys\n",
    "sys.path.append('../../../../src')\n",
    "import pandas\n",
    "import random\n",
    "import math\n",
    "from itertools import product\n",
    "from energiapy.components.temporal_scale import TemporalScale\n",
    "from energiapy.components.resource import Resource, VaryingResource\n",
    "from energiapy.components.process import Process, ProcessMode, VaryingProcess\n",
    "from energiapy.components.location import Location\n",
    "from energiapy.components.transport import Transport, VaryingTransport\n",
    "from energiapy.components.network import Network\n",
    "from energiapy.components.scenario import Scenario\n",
    "# from energiapy.model.constraints.demand import constraint_demand2\n",
    "from energiapy.components.result import Result\n",
    "from energiapy.model.formulate import formulate, Constraints, Objective\n",
    "from energiapy.plot import plot_results, plot_scenario, plot_location\n",
    "from energiapy.model.solve import solve\n",
    "from pyomo.environ import Param\n",
    "from energiapy.utils.scale_utils import scale_pyomo_set\n",
    "from energiapy.utils.scale_utils import scale_list, scale_tuple\n",
    "from energiapy.model.constraints.constraints import make_constraint, Cons\n",
    "from energiapy.model.formulate import constraint_export\n",
    "from functools import reduce\n",
    "import pickle\n",
    "from pyomo.environ import value as pyoval\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# _time_intervals = 7  # Number of time intervals in a planning horizon    (L_chi)\n",
    "_exec_scenarios = 4  # Number of execution scenarios                     (chi)\n",
    "\n",
    "M = 1e5  # Big M"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:37:05.457899Z",
     "start_time": "2024-11-05T16:37:05.440196Z"
    }
   },
   "id": "31d027989fe5b9f4",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_list(n_total: int, n: int):\n",
    "    return [1] * n + [0] * (n_total - n)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:37:05.473047Z",
     "start_time": "2024-11-05T16:37:05.461047Z"
    }
   },
   "id": "71c97b99b0dc329a",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_event_dict(n_total: int):\n",
    "    default_list = [1] * n_total\n",
    "\n",
    "    # If event names are same before '_'; they are considered mutually exclusive\n",
    "    event_dict = {\n",
    "        'cap2_1': {'prob': 0.06, 'factor': pandas.DataFrame(data={('loc2', 'com1_process'): create_list(n_total, 1)})},\n",
    "        'cap2_2': {'prob': 0.12, 'factor': pandas.DataFrame(data={('loc2', 'com1_process'): create_list(n_total, 2)})},\n",
    "        'cap2_3': {'prob': 0.17, 'factor': pandas.DataFrame(data={('loc2', 'com1_process'): create_list(n_total, 3)})},\n",
    "        'cap2_nd': {'prob': 0.65, 'factor': pandas.DataFrame(data={('loc2', 'com1_process'): default_list})},\n",
    "\n",
    "        'cap4_1': {'prob': 0.05, 'factor': pandas.DataFrame(data={('loc4', 'com1_process'): create_list(n_total, 1)})},\n",
    "        'cap4_2': {'prob': 0.15, 'factor': pandas.DataFrame(data={('loc4', 'com1_process'): create_list(n_total, 2)})},\n",
    "        'cap4_3': {'prob': 0.18, 'factor': pandas.DataFrame(data={('loc4', 'com1_process'): create_list(n_total, 3)})},\n",
    "        'cap4_nd': {'prob': 0.62, 'factor': pandas.DataFrame(data={('loc4', 'com1_process'): default_list})},\n",
    "\n",
    "        'cap7_1': {'prob': 0.04, 'factor': pandas.DataFrame(data={('loc7', 'com1_process'): create_list(n_total, 1)})},\n",
    "        'cap7_2': {'prob': 0.15, 'factor': pandas.DataFrame(data={('loc7', 'com1_process'): create_list(n_total, 2)})},\n",
    "        'cap7_3': {'prob': 0.2, 'factor': pandas.DataFrame(data={('loc7', 'com1_process'): create_list(n_total, 3)})},\n",
    "        'cap7_nd': {'prob': 0.61, 'factor': pandas.DataFrame(data={('loc7', 'com1_process'): default_list})},\n",
    "\n",
    "        'res1_1': {'prob': 0.06, 'factor': pandas.DataFrame(data={('loc1', 'com1_pur'): create_list(n_total, 1)})},\n",
    "        'res1_2': {'prob': 0.12, 'factor': pandas.DataFrame(data={('loc1', 'com1_pur'): create_list(n_total, 2)})},\n",
    "        'res1_3': {'prob': 0.19, 'factor': pandas.DataFrame(data={('loc1', 'com1_pur'): create_list(n_total, 3)})},\n",
    "        'res1_nd': {'prob': 0.63, 'factor': pandas.DataFrame(data={('loc1', 'com1_pur'): default_list})},\n",
    "\n",
    "        'res6_1': {'prob': 0.03, 'factor': pandas.DataFrame(data={('loc6', 'com1_pur'): create_list(n_total, 1)})},\n",
    "        'res6_2': {'prob': 0.16, 'factor': pandas.DataFrame(data={('loc6', 'com1_pur'): create_list(n_total, 2)})},\n",
    "        'res6_3': {'prob': 0.17, 'factor': pandas.DataFrame(data={('loc6', 'com1_pur'): create_list(n_total, 3)})},\n",
    "        'res6_nd': {'prob': 0.64, 'factor': pandas.DataFrame(data={('loc6', 'com1_pur'): default_list})},\n",
    "\n",
    "        'trans12_2': {'prob': 0.2, 'factor': pandas.DataFrame(data={('trans12', 'com1_loc1_out'): create_list(n_total, 2)})},\n",
    "        'trans12_nd': {'prob': 0.8, 'factor': pandas.DataFrame(data={('trans12', 'com1_loc1_out'): default_list})},\n",
    "\n",
    "        'trans25_2': {'prob': 0.3, 'factor': pandas.DataFrame(data={('trans25', 'com1_loc2_out'): create_list(n_total, 2)})},\n",
    "        'trans25_nd': {'prob': 0.7, 'factor': pandas.DataFrame(data={('trans25', 'com1_loc2_out'): default_list})},\n",
    "    }\n",
    "\n",
    "    return event_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:37:05.488183Z",
     "start_time": "2024-11-05T16:37:05.476052Z"
    }
   },
   "id": "412df4e69adec277",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to generate the scenario dictionary for n sets of events\n",
    "def create_scenario_dict(event_dict):\n",
    "    # Extract unique event prefixes (e.g., 'cap2', 'cap4', ...)\n",
    "    event_prefixes = set(key.split('_')[0] for key in event_dict)\n",
    "\n",
    "    # Group events by their prefixes\n",
    "    grouped_events = {prefix: [key for key in event_dict if key.startswith(prefix)] for prefix in event_prefixes}\n",
    "\n",
    "    # Create all possible combinations of events across the different groups\n",
    "    event_combinations = list(product(*grouped_events.values()))\n",
    "\n",
    "    scenario_dict = {}\n",
    "\n",
    "    # Iterate over all event combinations\n",
    "    for combination in event_combinations:\n",
    "        # Construct the scenario key\n",
    "        scenario_key = ' '.join(combination)\n",
    "\n",
    "        # Calculate the probability of this scenario\n",
    "        prob = 1\n",
    "        combined_factor = None\n",
    "\n",
    "        for event_key in combination:\n",
    "            # Multiply probabilities\n",
    "            prob *= event_dict[event_key]['prob']\n",
    "\n",
    "            # Combine factors (assumes they are pandas DataFrames)\n",
    "            if combined_factor is None:\n",
    "                combined_factor = event_dict[event_key]['factor'].copy()\n",
    "            else:\n",
    "                combined_factor = combined_factor.add(event_dict[event_key]['factor'], fill_value=0)\n",
    "\n",
    "        # Add to the scenario dictionary\n",
    "        scenario_dict[scenario_key] = {'prob': prob, 'factor': combined_factor}\n",
    "\n",
    "    return scenario_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:37:05.504010Z",
     "start_time": "2024-11-05T16:37:05.489687Z"
    }
   },
   "id": "a07bc450b868a55e",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def build_model(scen_df=pandas.DataFrame()):\n",
    "    default_df = pandas.DataFrame(data=[1] * _exec_scenarios)\n",
    "    scale_factor = 90\n",
    "\n",
    "    # Define temporal scales\n",
    "    scales = TemporalScale(discretization_list=[1, _exec_scenarios])\n",
    "\n",
    "    # ======================================================================================================================\n",
    "    # Declare resources/commodities\n",
    "    # ======================================================================================================================\n",
    "    com1_pur = Resource(name='com1_pur', cons_max=225*scale_factor, block={'imp': 1, 'urg': 1}, price=0.00,\n",
    "                        label='Commodity 1 consumed from outside the system',\n",
    "                        varying=[VaryingResource.DETERMINISTIC_AVAILABILITY])\n",
    "\n",
    "    com1_in = Resource(name='com1_in', label='Commodity 1 received')\n",
    "    com1_out = Resource(name='com1_out', label='Commodity 1 to be sent out')\n",
    "\n",
    "    com1_loc1_out = Resource(name='com1_loc1_out', label='Commodity 1 sent out from location 1')\n",
    "    com1_loc2_out = Resource(name='com1_loc2_out', label='Commodity 1 sent out from location 2')\n",
    "    com1_loc3_out = Resource(name='com1_loc3_out', label='Commodity 1 sent out from location 3')\n",
    "    com1_loc4_out = Resource(name='com1_loc4_out', label='Commodity 1 sent out from location 4')\n",
    "    com1_loc5_out = Resource(name='com1_loc5_out', label='Commodity 1 sent out from location 5')\n",
    "    com1_loc6_out = Resource(name='com1_loc6_out', label='Commodity 1 sent out from location 6')\n",
    "    com1_loc7_out = Resource(name='com1_loc7_out', label='Commodity 1 sent out from location 7')\n",
    "\n",
    "    com1_sold = Resource(name='com1_sold', revenue=0.00, demand=True, sell=True,\n",
    "                         label='Commodity 1 sold to outside the system')\n",
    "\n",
    "    # ======================================================================================================================\n",
    "    # Declare processes/storage capacities\n",
    "    # ======================================================================================================================\n",
    "    com1_process_capacity = 500*scale_factor\n",
    "\n",
    "    # prod_max = {0: 0.25*com1_process_capacity, 1: 0.5*com1_process_capacity, 2: 0.75*com1_process_capacity, 3: 0.95*com1_process_capacity, 4: com1_process_capacity}\n",
    "    # prod_min = {0: 0, 1: 0.25*com1_process_capacity, 2: 0.5*com1_process_capacity, 3: 0.75*com1_process_capacity, 4: 0.95*com1_process_capacity}\n",
    "    # rate_max = {0:1.25/2, 1: 1/2, 2: 0.75/2, 3: 0.5/2, 4: 0.25/2}\n",
    "    # mode_ramp = {(0,1): 5, (1,2): 5}\n",
    "\n",
    "    com1_procure = Process(name='procure com1', prod_max=com1_process_capacity, conversion={com1_pur: -1, com1_in: 1},\n",
    "                           capex=25/scale_factor, vopex=0.01, prod_min=0.01, label='Procure com1')\n",
    "    com1_sell = Process(name='sell com1', prod_max=com1_process_capacity, conversion={com1_out: -1, com1_sold: 1},\n",
    "                        capex=0.1/scale_factor, vopex=0.01, prod_min=0.01, label='Sell com1')\n",
    "    # com1_opt_procure = Process(name='procure optional com1', prod_max=75, conversion={com1_pur: -1, com1_in:1}, capex=10, vopex=0.1, prod_min=0.01, label='Procure optional com1')\n",
    "\n",
    "    com1_receive_loc1 = Process(name='com1_receive_loc1', prod_max=com1_process_capacity,\n",
    "                                conversion={com1_loc1_out: -1, com1_in: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                                label='Commodity 1 received from location 1')\n",
    "    com1_receive_loc2 = Process(name='com1_receive_loc2', prod_max=com1_process_capacity,\n",
    "                                conversion={com1_loc2_out: -1, com1_in: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                                label='Commodity 1 received from location 2')\n",
    "    com1_receive_loc3 = Process(name='com1_receive_loc3', prod_max=com1_process_capacity,\n",
    "                                conversion={com1_loc3_out: -1, com1_in: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                                label='Commodity 1 received from location 3')\n",
    "    com1_receive_loc4 = Process(name='com1_receive_loc4', prod_max=com1_process_capacity,\n",
    "                                conversion={com1_loc4_out: -1, com1_in: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                                label='Commodity 1 received from location 4')\n",
    "    com1_receive_loc5 = Process(name='com1_receive_loc5', prod_max=com1_process_capacity,\n",
    "                                conversion={com1_loc5_out: -1, com1_in: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                                label='Commodity 1 received from location 5')\n",
    "    com1_receive_loc6 = Process(name='com1_receive_loc6', prod_max=com1_process_capacity,\n",
    "                                conversion={com1_loc6_out: -1, com1_in: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                                label='Commodity 1 received from location 6')\n",
    "    com1_receive_loc7 = Process(name='com1_receive_loc7', prod_max=com1_process_capacity,\n",
    "                                conversion={com1_loc7_out: -1, com1_in: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                                label='Commodity 1 received from location 7')\n",
    "\n",
    "    com1_process = Process(name='com1_process', prod_max=com1_process_capacity, conversion={com1_in: -1, com1_out: 1},\n",
    "                           capex=5/scale_factor, vopex=0.01, prod_min=0.01, label='Process the commodity through the location',\n",
    "                           varying=[VaryingProcess.DETERMINISTIC_CAPACITY])\n",
    "\n",
    "    com1_store = Process(name='com1_store', prod_max=com1_process_capacity, capex=0.5/scale_factor, vopex=0.01, storage_capex=30/scale_factor, store_min=0.01,\n",
    "                         store_max=200*scale_factor, prod_min=0.01, label=\"Storage process\", storage=com1_in, storage_cost=0.02)\n",
    "\n",
    "    com1_loc1_send = Process(name='com1_loc1_send', prod_max=com1_process_capacity,\n",
    "                             conversion={com1_out: -1, com1_loc1_out: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                             label='Send commodity one from location 1')\n",
    "    com1_loc2_send = Process(name='com1_loc2_send', prod_max=com1_process_capacity,\n",
    "                             conversion={com1_out: -1, com1_loc2_out: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                             label='Send commodity one from location 2')\n",
    "    com1_loc3_send = Process(name='com1_loc3_send', prod_max=com1_process_capacity,\n",
    "                             conversion={com1_out: -1, com1_loc3_out: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                             label='Send commodity one from location 3')\n",
    "    com1_loc4_send = Process(name='com1_loc4_send', prod_max=com1_process_capacity,\n",
    "                             conversion={com1_out: -1, com1_loc4_out: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                             label='Send commodity one from location 4')\n",
    "    com1_loc5_send = Process(name='com1_loc5_send', prod_max=com1_process_capacity,\n",
    "                             conversion={com1_out: -1, com1_loc5_out: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                             label='Send commodity one from location 5')\n",
    "    com1_loc6_send = Process(name='com1_loc6_send', prod_max=com1_process_capacity,\n",
    "                             conversion={com1_out: -1, com1_loc6_out: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                             label='Send commodity one from location 6')\n",
    "    com1_loc7_send = Process(name='com1_loc7_send', prod_max=com1_process_capacity,\n",
    "                             conversion={com1_out: -1, com1_loc7_out: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                             label='Send commodity one from location 7')\n",
    "\n",
    "    # ======================================================================================================================\n",
    "    # Declare locations/warehouses\n",
    "    # ======================================================================================================================\n",
    "    loc1 = Location(name='loc1',\n",
    "                    processes={com1_procure, com1_receive_loc2, com1_receive_loc3, com1_process, com1_store,\n",
    "                               com1_loc1_send}, label=\"Location 1\",\n",
    "                    scales=scales, demand_scale_level=1, capacity_scale_level=1, availability_scale_level=1,\n",
    "                    availability_factor={\n",
    "                        com1_pur: scen_df[[('loc1', 'com1_pur')]] if ('loc1', 'com1_pur') in scen_df else default_df})\n",
    "\n",
    "    loc2 = Location(name='loc2',\n",
    "                    processes={com1_receive_loc1, com1_receive_loc4, com1_receive_loc5, com1_process, com1_store,\n",
    "                               com1_loc2_send}, label=\"Location 2\", scales=scales, demand_scale_level=1,\n",
    "                    capacity_scale_level=1, availability_scale_level=1,\n",
    "                    capacity_factor={com1_process: scen_df[[('loc2', 'com1_process')]] if ('loc2',\n",
    "                                                                                           'com1_process') in scen_df else default_df})\n",
    "\n",
    "    loc3 = Location(name='loc3',\n",
    "                    processes={com1_receive_loc1, com1_receive_loc4, com1_process, com1_store, com1_loc3_send},\n",
    "                    label=\"Location 3\", scales=scales, demand_scale_level=1, capacity_scale_level=1,\n",
    "                    availability_scale_level=1)\n",
    "\n",
    "    loc4 = Location(name='loc4', processes={com1_receive_loc2, com1_receive_loc3, com1_receive_loc6, com1_receive_loc5,\n",
    "                                            com1_receive_loc7, com1_process, com1_store, com1_loc4_send},\n",
    "                    label=\"Location 4\", scales=scales, demand_scale_level=1, capacity_scale_level=1,\n",
    "                    availability_scale_level=1,\n",
    "                    capacity_factor={com1_process: scen_df[[('loc4', 'com1_process')]] if ('loc4',\n",
    "                                                                                           'com1_process') in scen_df else default_df})\n",
    "\n",
    "    loc5 = Location(name='loc5',\n",
    "                    processes={com1_receive_loc2, com1_receive_loc4, com1_receive_loc7, com1_process, com1_store,\n",
    "                               com1_loc5_send, com1_sell}, label=\"Location 5\", scales=scales, demand_scale_level=1,\n",
    "                    capacity_scale_level=1, availability_scale_level=1)\n",
    "\n",
    "    loc6 = Location(name='loc6', processes={com1_procure, com1_receive_loc4, com1_process, com1_store, com1_loc6_send},\n",
    "                    label=\"Location 6\", scales=scales, demand_scale_level=1, capacity_scale_level=1,\n",
    "                    availability_scale_level=1,\n",
    "                    availability_factor={\n",
    "                        com1_pur: scen_df[[('loc6', 'com1_pur')]] if ('loc6', 'com1_pur') in scen_df else default_df})\n",
    "\n",
    "    loc7 = Location(name='loc7',\n",
    "                    processes={com1_receive_loc4, com1_receive_loc5, com1_process, com1_store, com1_loc7_send},\n",
    "                    label=\"Location 7\", scales=scales, demand_scale_level=1, capacity_scale_level=1,\n",
    "                    availability_scale_level=1,\n",
    "                    capacity_factor={com1_process: scen_df[[('loc7', 'com1_process')]] if ('loc7',\n",
    "                                                                                           'com1_process') in scen_df else default_df})\n",
    "\n",
    "    # ======================================================================================================================\n",
    "    # Declare transport/trucks\n",
    "    # ======================================================================================================================\n",
    "\n",
    "    truck_cap12 = 280*scale_factor\n",
    "    truck_cap13 = 270*scale_factor\n",
    "    truck_cap24 = 450*scale_factor\n",
    "    truck_cap25 = 270*scale_factor\n",
    "    truck_cap34 = 270*scale_factor\n",
    "    truck_cap45 = 500*scale_factor\n",
    "    truck_cap47 = 360*scale_factor\n",
    "    truck_cap64 = 450*scale_factor\n",
    "    truck_cap75 = 360*scale_factor\n",
    "\n",
    "    truck12 = Transport(name='truck12', resources={com1_loc1_out}, trans_max=truck_cap12,\n",
    "                        label='Truck from location 1 to 2', capex=0.5/scale_factor, vopex=0.05, trans_min=0.01, varying=[VaryingTransport.DETERMINISTIC_CAPACITY])\n",
    "    # truck21 = Transport(name='truck21', resources={com1_loc2_out}, trans_max=truck_cap12,\n",
    "    #                     label='Truck from location 2 to 1', capex=0.0001, vopex=0.05, trans_min=0.01)\n",
    "\n",
    "    truck13 = Transport(name='truck13', resources={com1_loc1_out}, trans_max=truck_cap13,\n",
    "                        label='Truck from location 1 to 3', capex=0.3/scale_factor, vopex=0.03, trans_min=0.01, varying=[VaryingTransport.DETERMINISTIC_CAPACITY])\n",
    "    # truck31 = Transport(name='truck31', resources={com1_loc3_out}, trans_max=truck_cap13,\n",
    "    #                     label='Truck from location 3 to 1', capex=0.0001, vopex=0.03, trans_min=0.01)\n",
    "\n",
    "    truck24 = Transport(name='truck24', resources={com1_loc2_out}, trans_max=truck_cap24,\n",
    "                        label='Truck from location 2 to 4', capex=0.5/scale_factor, vopex=0.05, trans_min=0.01, varying=[VaryingTransport.DETERMINISTIC_CAPACITY])\n",
    "    # truck42 = Transport(name='truck42', resources={com1_loc4_out}, trans_max=truck_cap24,\n",
    "    #                     label='Truck from location 4 to 2', capex=0.0001, vopex=0.05, trans_min=0.01)\n",
    "\n",
    "    truck25 = Transport(name='truck25', resources={com1_loc2_out}, trans_max=truck_cap25,\n",
    "                        label='Truck from location 2 to 5', capex=0.3/scale_factor, vopex=0.03, trans_min=0.01, varying=[VaryingTransport.DETERMINISTIC_CAPACITY])\n",
    "    # truck52 = Transport(name='truck52', resources={com1_loc5_out}, trans_max=truck_cap25,\n",
    "    #                     label='Truck from location 5 to 2', capex=0.0001, vopex=0.03, trans_min=0.01)\n",
    "\n",
    "    truck34 = Transport(name='truck34', resources={com1_loc3_out}, trans_max=truck_cap34,\n",
    "                        label='Truck from location 3 to 4', capex=0.2/scale_factor, vopex=0.02, trans_min=0.01, varying=[VaryingTransport.DETERMINISTIC_CAPACITY])\n",
    "    # truck43 = Transport(name='truck43', resources={com1_loc4_out}, trans_max=truck_cap34,\n",
    "    #                     label='Truck from location 4 to 3', capex=0.0001, vopex=0.02, trans_min=0.01)\n",
    "\n",
    "    truck45 = Transport(name='truck45', resources={com1_loc4_out}, trans_max=truck_cap45,\n",
    "                        label='Truck from location 4 to 5', capex=1/scale_factor, vopex=0.1, trans_min=0.01, varying=[VaryingTransport.DETERMINISTIC_CAPACITY])\n",
    "    # truck54 = Transport(name='truck54', resources={com1_loc5_out}, trans_max=truck_cap45,\n",
    "    #                     label='Truck from location 5 to 4', capex=0.0001, vopex=0.1, trans_min=0.01)\n",
    "\n",
    "    truck47 = Transport(name='truck47', resources={com1_loc4_out}, trans_max=truck_cap47,\n",
    "                        label='Truck from location 4 to 7', capex=0.4/scale_factor, vopex=0.04, trans_min=0.01, varying=[VaryingTransport.DETERMINISTIC_CAPACITY])\n",
    "    # truck74 = Transport(name='truck74', resources={com1_loc7_out}, trans_max=truck_cap47,\n",
    "    #                     label='Truck from location 7 to 4', capex=0.0001, vopex=0.04, trans_min=0.01)\n",
    "\n",
    "    truck64 = Transport(name='truck64', resources={com1_loc6_out}, trans_max=truck_cap64,\n",
    "                        label='Truck from location 6 to 4', capex=0.5/scale_factor, vopex=0.05, trans_min=0.01, varying=[VaryingTransport.DETERMINISTIC_CAPACITY])\n",
    "    # truck46 = Transport(name='truck46', resources={com1_loc4_out}, trans_max=truck_cap64,\n",
    "    #                     label='Truck from location 4 to 6', capex=0.0001, vopex=0.05, trans_min=0.01)\n",
    "\n",
    "    truck75 = Transport(name='truck75', resources={com1_loc7_out}, trans_max=truck_cap75,\n",
    "                        label='Truck from location 7 to 5', capex=0.4/scale_factor, vopex=0.04, trans_min=0.01, varying=[VaryingTransport.DETERMINISTIC_CAPACITY])\n",
    "    # truck57 = Transport(name='truck57', resources={com1_loc5_out}, trans_max=truck_cap75,\n",
    "    #                     label='Truck from location 5 to 7', capex=0.0001, vopex=0.04, trans_min=0.01)\n",
    "\n",
    "    # ======================================================================================================================\n",
    "    # Declare network\n",
    "    # ======================================================================================================================\n",
    "\n",
    "    transport_matrix = [\n",
    "        [[], [truck12], [truck13], [], [], [], []],  # source: location 1\n",
    "        [[], [], [], [truck24], [truck25], [], []],  # source: location 2\n",
    "        [[], [], [], [truck34], [], [], []],  # source: location 3\n",
    "        [[], [], [], [], [truck45], [], [truck47]],  # source: location 4\n",
    "        [[], [], [], [], [], [], []],  # source: location 5\n",
    "        [[], [], [], [truck64], [], [], []],  # source: location 6\n",
    "        [[], [], [], [], [truck75], [], []]  # source: location 7\n",
    "    ]\n",
    "\n",
    "    # transport_matrix = [\n",
    "    #     [[], [truck12], [truck13], [], [], [], []],  # source: location 1\n",
    "    #     [[truck21], [], [], [truck24], [truck25], [], []],  # source: location 2\n",
    "    #     [[truck31], [], [], [truck34], [], [], []],  # source: location 3\n",
    "    #     [[], [truck42], [truck43], [], [truck45], [truck46], [truck47]],  # source: location 4\n",
    "    #     [[], [truck52], [], [truck54], [], [], [truck57]],  # source: location 5\n",
    "    #     [[], [], [], [truck64], [], [], []],  # source: location 6\n",
    "    #     [[], [], [], [truck74], [truck75], [], []]  # source: location 7\n",
    "    # ]\n",
    "\n",
    "    distance_matrix = [\n",
    "        [0, 55, 196, M, M, M, M],\n",
    "        [55, 0, M, 163, 112, M, 134],\n",
    "        [196, M, 0, 63, M, M, M],\n",
    "        [M, 163, 63, 0, 95, 117, 88],\n",
    "        [M, 112, M, 95, 0, M, 134],\n",
    "        [M, M, M, 117, M, 0, M],\n",
    "        [M, 134, M, 88, 134, M, 0]\n",
    "    ]\n",
    "\n",
    "    locset = [loc1, loc2, loc3, loc4, loc5, loc6, loc7]\n",
    "\n",
    "    sources = locset\n",
    "    sinks = locset\n",
    "\n",
    "    network = Network(name='Network', scales=scales, source_locations=sources, sink_locations=sinks,\n",
    "                      transport_matrix=transport_matrix, distance_matrix=distance_matrix, transport_capacity_scale_level=1,\n",
    "                      transport_capacity_factor={(loc1, loc2): {truck12: scen_df[[('trans12', 'com1_loc1_out')]] if ('trans12', 'com1_loc1_out') in scen_df else default_df},\n",
    "                                                 (loc1, loc3): {truck13: scen_df[[('trans13', 'com1_loc1_out')]] if ('trans13', 'com1_loc1_out') in scen_df else default_df},\n",
    "                                                 (loc2, loc4): {truck24: scen_df[[('trans24', 'com1_loc2_out')]] if ('trans24', 'com1_loc2_out') in scen_df else default_df},\n",
    "                                                 (loc2, loc5): {truck25: scen_df[[('trans25', 'com1_loc2_out')]] if ('trans25', 'com1_loc2_out') in scen_df else default_df},\n",
    "                                                 (loc3, loc4): {truck34: scen_df[[('trans34', 'com1_loc3_out')]] if ('trans34', 'com1_loc3_out') in scen_df else default_df},\n",
    "                                                 (loc4, loc5): {truck45: scen_df[[('trans45', 'com1_loc4_out')]] if ('trans45', 'com1_loc4_out') in scen_df else default_df},\n",
    "                                                 (loc4, loc7): {truck47: scen_df[[('trans47', 'com1_loc4_out')]] if ('trans47', 'com1_loc4_out') in scen_df else default_df},\n",
    "                                                 (loc6, loc4): {truck64: scen_df[[('trans64', 'com1_loc6_out')]] if ('trans64', 'com1_loc6_out') in scen_df else default_df},\n",
    "                                                 (loc7, loc5): {truck75: scen_df[[('trans75', 'com1_loc7_out')]] if ('trans75', 'com1_loc7_out') in scen_df else default_df},\n",
    "                                                 })\n",
    "\n",
    "    # ======================================================================================================================\n",
    "    # Declare scenario\n",
    "    # ======================================================================================================================\n",
    "\n",
    "    daily_demand = 400*scale_factor\n",
    "    demand_penalty = 20\n",
    "\n",
    "    demand_dict = {i: {com1_sold: daily_demand} if i == loc5 else {com1_sold: 0} for i in locset}\n",
    "    demand_penalty_dict = {i: {com1_sold: demand_penalty} if i == loc5 else {com1_sold: 0} for i in locset}\n",
    "\n",
    "    scenario = Scenario(name='scenario', scales=scales, scheduling_scale_level=1, network_scale_level=0,\n",
    "                        purchase_scale_level=1, availability_scale_level=1, demand_scale_level=1,\n",
    "                        capacity_scale_level=1, network=network, demand=demand_dict, demand_penalty=demand_penalty_dict,\n",
    "                        label='Stochastic scenario with Multiple Locations')\n",
    "\n",
    "    if scen_df.empty:\n",
    "        # ======================================================================================================================\n",
    "        # Declare problem\n",
    "        # ======================================================================================================================\n",
    "\n",
    "        problem_mincost = formulate(scenario=scenario,\n",
    "                                    constraints={Constraints.COST, Constraints.TRANSPORT, Constraints.RESOURCE_BALANCE,\n",
    "                                                 Constraints.INVENTORY, Constraints.PRODUCTION, Constraints.DEMAND,\n",
    "                                                 Constraints.NETWORK},\n",
    "                                    demand_sign='eq', objective=Objective.COST_W_DEMAND_PENALTY)\n",
    "\n",
    "        scale_iter = scale_tuple(instance=problem_mincost, scale_levels=scenario.network_scale_level + 1)\n",
    "        # capex_process = sum(problem_mincost.Capex_network[scale_] for scale_ in scale_iter)\n",
    "        # cost_trans_capex = sum(problem_mincost.Capex_transport_network[scale_] for scale_ in scale_iter)\n",
    "        \n",
    "        problem_mincost.first_stage_cost = Var(within=NonNegativeReals, doc='First Stage Cost')\n",
    "        \n",
    "        def first_stage_cost_rule(instance):\n",
    "            return (instance.first_stage_cost == sum(instance.Capex_network[scale_] for scale_ in scale_iter) + \n",
    "                                                sum(instance.Capex_transport_network[scale_] for scale_ in scale_iter))\n",
    "        \n",
    "        problem_mincost.constraint_first_stage_cost = Constraint(rule=first_stage_cost_rule)\n",
    "\n",
    "        return scenario, problem_mincost\n",
    "\n",
    "    else:\n",
    "        return scenario"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:37:05.535395Z",
     "start_time": "2024-11-05T16:37:05.505023Z"
    }
   },
   "id": "635bd2e5e41178b9",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def build_smodel(scen_df=pandas.DataFrame()):\n",
    "\n",
    "    scenario = build_model(scen_df)\n",
    "    # ======================================================================================================================\n",
    "    # Declare problem\n",
    "    # ======================================================================================================================\n",
    "\n",
    "    problem_mincost = formulate(scenario=scenario,\n",
    "                                constraints={Constraints.COST, Constraints.TRANSPORT, Constraints.RESOURCE_BALANCE,\n",
    "                                             Constraints.INVENTORY, Constraints.PRODUCTION, Constraints.DEMAND,\n",
    "                                             Constraints.NETWORK},\n",
    "                                demand_sign='eq', objective=Objective.COST_W_DEMAND_PENALTY)\n",
    "\n",
    "    scale_iter = scale_tuple(instance=problem_mincost, scale_levels=scenario.network_scale_level + 1)\n",
    "    # capex_process = sum(problem_mincost.Capex_network[scale_] for scale_ in scale_iter)\n",
    "    # cost_trans_capex = sum(problem_mincost.Capex_transport_network[scale_] for scale_ in scale_iter)\n",
    "    \n",
    "    problem_mincost.first_stage_cost = Var(within=NonNegativeReals, doc='First Stage Cost')\n",
    "    \n",
    "    def first_stage_cost_rule(instance):\n",
    "        return (instance.first_stage_cost == sum(instance.Capex_network[scale_] for scale_ in scale_iter) + \n",
    "                                            sum(instance.Capex_transport_network[scale_] for scale_ in scale_iter))\n",
    "    \n",
    "    problem_mincost.constraint_first_stage_cost = Constraint(rule=first_stage_cost_rule)\n",
    "\n",
    "    return scenario, problem_mincost"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:37:05.551438Z",
     "start_time": "2024-11-05T16:37:05.536439Z"
    }
   },
   "id": "4ffe297bc646225e",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def scenario_creator(scen_name, **kwargs):\n",
    "    scen_dict = kwargs.get('scenario_dict')\n",
    "    scen, model = build_smodel(scen_df=scen_dict[scen_name]['factor'])\n",
    "    sputils.attach_root_node(model, model.first_stage_cost,\n",
    "                             [model.X_P, model.Cap_P, model.X_S, model.Cap_S, model.X_F, model.Cap_F])\n",
    "    model._mpisppy_probability = scen_dict[scen_name]['prob']\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:37:05.566656Z",
     "start_time": "2024-11-05T16:37:05.552439Z"
    }
   },
   "id": "6a6287d77eb8adfe",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "solver_options = {\n",
    "    'MIPGap': 0.005,\n",
    "    # 'TimeLimit': 60 * 15,\n",
    "    'Heuristics': 0.20\n",
    "}\n",
    "\n",
    "# event_dict = create_event_dict(n_total=_exec_scenarios)\n",
    "# scenario_dict = create_scenario_dict(event_dict=event_dict)\n",
    "# scenario_names = list(scenario_dict.keys())\n",
    "# \n",
    "# with open('scenario_dict_HP.pkl', 'wb') as file:\n",
    "#     pickle.dump(scenario_dict, file)\n",
    "# \n",
    "# # print(*scenario_names, sep='\\n')\n",
    "# print(f\"Sum of probabilities of all scenarios: {sum(scenario_dict[scen]['prob'] for scen in scenario_dict):.6f}\")\n",
    "# print(f'Number of considered scenarios: {len(scenario_names)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:37:05.582227Z",
     "start_time": "2024-11-05T16:37:05.567695Z"
    }
   },
   "id": "1a6bc421544d5f19",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of probabilities of all scenarios: 1.000000\n",
      "Number of considered scenarios: 4096\n"
     ]
    }
   ],
   "source": [
    "with open('scenario_dict_HP.pkl', 'rb') as file:\n",
    "    load_scenario_dict = pickle.load(file)\n",
    "\n",
    "load_scenario_names = list(load_scenario_dict.keys())\n",
    "\n",
    "print(f\"Sum of probabilities of all scenarios: {sum(load_scenario_dict[scen]['prob'] for scen in load_scenario_dict):.6f}\")\n",
    "print(f'Number of considered scenarios: {len(load_scenario_names)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:37:06.452426Z",
     "start_time": "2024-11-05T16:37:05.583260Z"
    }
   },
   "id": "be5cfe5fa6c0f077",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "PI_pkl_folder = r'PI_p'\n",
    "# FD_pkl_folder = r'FD_HP_pkl'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:37:06.468214Z",
     "start_time": "2024-11-05T16:37:06.453428Z"
    }
   },
   "id": "758588516ab0e8cd",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constraint process capex\n",
      "constraint process fopex\n",
      "constraint process vopex\n",
      "constraint process incidental\n",
      "constraint location capex\n",
      "constraint location fopex\n",
      "constraint storage cost\n",
      "constraint storage capex\n",
      "constraint storage cost location\n",
      "constraint storage cost network\n",
      "constraint production mode\n",
      "constraint inventory balance\n",
      "constraint inventory network\n",
      "constraint resource export\n",
      "constraint transport export\n",
      "constraint export\n",
      "constraint transport capex\n",
      "constraint transport network capex\n",
      "constraint transport export network\n",
      "constraint transport vopex\n",
      "constraint transport network vopex\n",
      "constraint transport fopex\n",
      "constraint transport network fopex\n",
      "constraint transport capacity UB no bin\n",
      "constraint transport capacity LB no bin\n",
      "constraint storage facility\n",
      "constraint production facility\n",
      "constraint min production facility\n",
      "constraint min storage facility\n",
      "constraint transport capacity UB\n",
      "constraint transport capacity LB\n",
      "constraint demand penalty\n",
      "constraint demand penalty location\n",
      "constraint demand penalty network\n",
      "constraint demand penalty cost\n",
      "constraint demand penalty cost location\n",
      "constraint demand penalty cost network\n",
      "objective cost w demand penalty\n"
     ]
    }
   ],
   "source": [
    "# Deterministic Scenarios for Perfect Information\n",
    "exCost_PI = 0\n",
    "counter = 0\n",
    "results_PI = dict()\n",
    "PI_output_dict=dict()\n",
    "scen_PI, model_PI = build_model()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:37:06.720491Z",
     "start_time": "2024-11-05T16:37:06.469217Z"
    }
   },
   "id": "a172772ea98763c2",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:37:06.736504Z",
     "start_time": "2024-11-05T16:37:06.721493Z"
    }
   },
   "id": "ca63db143337cac9",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constraint export\n",
      "Set parameter MIPGap to value 0.005\n",
      "Set parameter Heuristics to value 0.2\n",
      "Set parameter QCPDual to value 1\n",
      "Gurobi Optimizer version 11.0.1 build v11.0.1rc0 (win64 - Windows 11.0 (22631.2))\n",
      "\n",
      "CPU model: 13th Gen Intel(R) Core(TM) i7-13700, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 16 physical cores, 24 logical processors, using up to 24 threads\n",
      "\n",
      "Optimize a model with 9763 rows, 18517 columns and 14982 nonzeros\n",
      "Model fingerprint: 0x53cd0e47\n",
      "Variable types: 17936 continuous, 581 integer (581 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-03, 5e+04]\n",
      "  Objective range  [1e+00, 2e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [2e+04, 5e+04]\n",
      "Found heuristic solution: objective 2880000.0000\n",
      "Presolve removed 9565 rows and 18389 columns\n",
      "Presolve time: 0.01s\n",
      "Presolved: 198 rows, 128 columns, 435 nonzeros\n",
      "Variable types: 82 continuous, 46 integer (46 binary)\n",
      "\n",
      "Root relaxation: objective 2.564543e+06, 61 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0    2564542.5000 2564542.50  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (61 simplex iterations) in 0.02 seconds (0.02 work units)\n",
      "Thread count was 24 (of 24 available processors)\n",
      "\n",
      "Solution count 2: 2.56454e+06 2.88e+06 \n",
      "\n",
      "Optimal solution found (tolerance 5.00e-03)\n",
      "Best objective 2.564542500000e+06, best bound 2.564542500000e+06, gap 0.0000%\n",
      "WARNING: Cannot get duals for MIP.\n",
      "######################## Finished solving cap4_1 cap7_1 trans12_2 res1_1 res6_1 trans25_2 cap2_1 (1 of 4096) ########################\n",
      "constraint export\n",
      "Set parameter MIPGap to value 0.005\n",
      "Set parameter Heuristics to value 0.2\n",
      "Set parameter QCPDual to value 1\n",
      "Gurobi Optimizer version 11.0.1 build v11.0.1rc0 (win64 - Windows 11.0 (22631.2))\n",
      "\n",
      "CPU model: 13th Gen Intel(R) Core(TM) i7-13700, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 16 physical cores, 24 logical processors, using up to 24 threads\n",
      "\n",
      "Optimize a model with 9763 rows, 18517 columns and 14983 nonzeros\n",
      "Model fingerprint: 0xdf39bd0e\n",
      "Variable types: 17936 continuous, 581 integer (581 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-03, 5e+04]\n",
      "  Objective range  [1e+00, 2e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [2e+04, 5e+04]\n",
      "Found heuristic solution: objective 2880000.0000\n",
      "Presolve removed 9550 rows and 18379 columns\n",
      "Presolve time: 0.01s\n",
      "Presolved: 213 rows, 138 columns, 480 nonzeros\n",
      "Variable types: 91 continuous, 47 integer (47 binary)\n",
      "\n",
      "Root relaxation: objective 2.561040e+06, 76 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "*    0     0               0    2561040.0000 2561040.00  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (76 simplex iterations) in 0.02 seconds (0.02 work units)\n",
      "Thread count was 24 (of 24 available processors)\n",
      "\n",
      "Solution count 2: 2.56104e+06 2.88e+06 \n",
      "\n",
      "Optimal solution found (tolerance 5.00e-03)\n",
      "Best objective 2.561040000000e+06, best bound 2.561040000000e+06, gap 0.0000%\n",
      "WARNING: Cannot get duals for MIP.\n",
      "######################## Finished solving cap4_1 cap7_1 trans12_2 res1_1 res6_1 trans25_2 cap2_2 (2 of 4096) ########################\n",
      "constraint export\n",
      "Set parameter MIPGap to value 0.005\n",
      "Set parameter Heuristics to value 0.2\n",
      "Set parameter QCPDual to value 1\n",
      "Gurobi Optimizer version 11.0.1 build v11.0.1rc0 (win64 - Windows 11.0 (22631.2))\n",
      "\n",
      "CPU model: 13th Gen Intel(R) Core(TM) i7-13700, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 16 physical cores, 24 logical processors, using up to 24 threads\n",
      "\n",
      "Optimize a model with 9763 rows, 18517 columns and 14984 nonzeros\n",
      "Model fingerprint: 0xde14abae\n",
      "Variable types: 17936 continuous, 581 integer (581 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-03, 5e+04]\n",
      "  Objective range  [1e+00, 2e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [2e+04, 5e+04]\n",
      "Found heuristic solution: objective 2880000.0000\n",
      "Presolve removed 9550 rows and 18379 columns\n",
      "Presolve time: 0.01s\n",
      "Presolved: 213 rows, 138 columns, 480 nonzeros\n",
      "Variable types: 91 continuous, 47 integer (47 binary)\n",
      "\n",
      "Root relaxation: objective 2.561040e+06, 76 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0    2561040.0000 2561040.00  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (76 simplex iterations) in 0.02 seconds (0.02 work units)\n",
      "Thread count was 24 (of 24 available processors)\n",
      "\n",
      "Solution count 2: 2.56104e+06 2.88e+06 \n",
      "Optimal solution found (tolerance 5.00e-03)\n",
      "Best objective 2.561040000000e+06, best bound 2.561040000000e+06, gap 0.0000%\n",
      "WARNING: Cannot get duals for MIP.\n",
      "######################## Finished solving cap4_1 cap7_1 trans12_2 res1_1 res6_1 trans25_2 cap2_3 (3 of 4096) ########################\n",
      "constraint export\n",
      "Set parameter MIPGap to value 0.005\n",
      "Set parameter Heuristics to value 0.2\n",
      "Set parameter QCPDual to value 1\n",
      "Gurobi Optimizer version 11.0.1 build v11.0.1rc0 (win64 - Windows 11.0 (22631.2))\n",
      "\n",
      "CPU model: 13th Gen Intel(R) Core(TM) i7-13700, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 16 physical cores, 24 logical processors, using up to 24 threads\n",
      "\n",
      "Optimize a model with 9763 rows, 18517 columns and 14985 nonzeros\n",
      "Model fingerprint: 0x8158d8c4\n",
      "Variable types: 17936 continuous, 581 integer (581 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-03, 5e+04]\n",
      "  Objective range  [1e+00, 2e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [2e+04, 5e+04]\n",
      "Found heuristic solution: objective 2880000.0000\n",
      "Presolve removed 9550 rows and 18379 columns\n",
      "Presolve time: 0.01s\n",
      "Presolved: 213 rows, 138 columns, 480 nonzeros\n",
      "Variable types: 91 continuous, 47 integer (47 binary)\n",
      "\n",
      "Root relaxation: objective 2.561040e+06, 76 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0    2561040.0000 2561040.00  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (76 simplex iterations) in 0.02 seconds (0.02 work units)\n",
      "Thread count was 24 (of 24 available processors)\n",
      "\n",
      "Solution count 2: 2.56104e+06 2.88e+06 \n",
      "\n",
      "Optimal solution found (tolerance 5.00e-03)\n",
      "Best objective 2.561040000000e+06, best bound 2.561040000000e+06, gap 0.0000%\n",
      "WARNING: Cannot get duals for MIP.\n",
      "######################## Finished solving cap4_1 cap7_1 trans12_2 res1_1 res6_1 trans25_2 cap2_nd (4 of 4096) ########################\n",
      "constraint export\n",
      "Set parameter MIPGap to value 0.005\n",
      "Set parameter Heuristics to value 0.2\n",
      "Set parameter QCPDual to value 1\n",
      "Gurobi Optimizer version 11.0.1 build v11.0.1rc0 (win64 - Windows 11.0 (22631.2))\n",
      "\n",
      "CPU model: 13th Gen Intel(R) Core(TM) i7-13700, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 16 physical cores, 24 logical processors, using up to 24 threads\n",
      "\n",
      "Optimize a model with 9763 rows, 18517 columns and 14984 nonzeros\n",
      "Model fingerprint: 0x5f1c9194\n",
      "Variable types: 17936 continuous, 581 integer (581 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-03, 5e+04]\n",
      "  Objective range  [1e+00, 2e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [2e+04, 5e+04]\n",
      "Found heuristic solution: objective 2880000.0000\n",
      "Presolve removed 9565 rows and 18389 columns\n",
      "Presolve time: 0.01s\n",
      "Presolved: 198 rows, 128 columns, 435 nonzeros\n",
      "Variable types: 82 continuous, 46 integer (46 binary)\n",
      "\n",
      "Root relaxation: objective 2.564543e+06, 61 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0    2564542.5000 2564542.50  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (61 simplex iterations) in 0.02 seconds (0.02 work units)\n",
      "Thread count was 24 (of 24 available processors)\n",
      "\n",
      "Solution count 2: 2.56454e+06 2.88e+06 \n",
      "\n",
      "Optimal solution found (tolerance 5.00e-03)\n",
      "Best objective 2.564542500000e+06, best bound 2.564542500000e+06, gap 0.0000%\n",
      "WARNING: Cannot get duals for MIP.\n",
      "######################## Finished solving cap4_1 cap7_1 trans12_2 res1_1 res6_1 trans25_nd cap2_1 (5 of 4096) ########################\n",
      "constraint export\n",
      "Set parameter MIPGap to value 0.005\n",
      "Set parameter Heuristics to value 0.2\n",
      "Set parameter QCPDual to value 1\n",
      "Gurobi Optimizer version 11.0.1 build v11.0.1rc0 (win64 - Windows 11.0 (22631.2))\n",
      "\n",
      "CPU model: 13th Gen Intel(R) Core(TM) i7-13700, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 16 physical cores, 24 logical processors, using up to 24 threads\n",
      "\n",
      "Optimize a model with 9763 rows, 18517 columns and 14985 nonzeros\n",
      "Model fingerprint: 0x1deb8f98\n",
      "Variable types: 17936 continuous, 581 integer (581 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-03, 5e+04]\n",
      "  Objective range  [1e+00, 2e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [2e+04, 5e+04]\n",
      "Found heuristic solution: objective 2880000.0000\n",
      "Presolve removed 9550 rows and 18379 columns\n",
      "Presolve time: 0.01s\n",
      "Presolved: 213 rows, 138 columns, 480 nonzeros\n",
      "Variable types: 91 continuous, 47 integer (47 binary)\n",
      "\n",
      "Root relaxation: objective 2.561040e+06, 76 iterations, 0.00 seconds (0.00 work units)\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0    2561040.0000 2561040.00  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (76 simplex iterations) in 0.02 seconds (0.02 work units)\n",
      "Thread count was 24 (of 24 available processors)\n",
      "\n",
      "Solution count 2: 2.56104e+06 2.88e+06 \n",
      "\n",
      "Optimal solution found (tolerance 5.00e-03)\n",
      "Best objective 2.561040000000e+06, best bound 2.561040000000e+06, gap 0.0000%\n",
      "WARNING: Cannot get duals for MIP.\n",
      "######################## Finished solving cap4_1 cap7_1 trans12_2 res1_1 res6_1 trans25_nd cap2_2 (6 of 4096) ########################\n",
      "constraint export\n",
      "Set parameter MIPGap to value 0.005\n",
      "Set parameter Heuristics to value 0.2\n",
      "Set parameter QCPDual to value 1\n",
      "Gurobi Optimizer version 11.0.1 build v11.0.1rc0 (win64 - Windows 11.0 (22631.2))\n",
      "\n",
      "CPU model: 13th Gen Intel(R) Core(TM) i7-13700, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 16 physical cores, 24 logical processors, using up to 24 threads\n",
      "\n",
      "Optimize a model with 9763 rows, 18517 columns and 14986 nonzeros\n",
      "Model fingerprint: 0xbd4bc128\n",
      "Variable types: 17936 continuous, 581 integer (581 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-03, 5e+04]\n",
      "  Objective range  [1e+00, 2e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [2e+04, 5e+04]\n",
      "Found heuristic solution: objective 2880000.0000\n",
      "Presolve removed 9538 rows and 18373 columns\n",
      "Presolve time: 0.01s\n",
      "Presolved: 225 rows, 144 columns, 511 nonzeros\n",
      "Variable types: 97 continuous, 47 integer (47 binary)\n",
      "\n",
      "Root relaxation: objective 2.561040e+06, 86 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0    2561040.0000 2561040.00  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (86 simplex iterations) in 0.02 seconds (0.02 work units)\n",
      "Thread count was 24 (of 24 available processors)\n",
      "\n",
      "Solution count 2: 2.56104e+06 2.88e+06 \n",
      "\n",
      "Optimal solution found (tolerance 5.00e-03)\n",
      "Best objective 2.561040000000e+06, best bound 2.561040000000e+06, gap 0.0000%\n",
      "WARNING: Cannot get duals for MIP.\n",
      "######################## Finished solving cap4_1 cap7_1 trans12_2 res1_1 res6_1 trans25_nd cap2_3 (7 of 4096) ########################\n",
      "constraint export\n",
      "Set parameter MIPGap to value 0.005\n",
      "Set parameter Heuristics to value 0.2\n",
      "Set parameter QCPDual to value 1\n",
      "Gurobi Optimizer version 11.0.1 build v11.0.1rc0 (win64 - Windows 11.0 (22631.2))\n",
      "\n",
      "CPU model: 13th Gen Intel(R) Core(TM) i7-13700, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 16 physical cores, 24 logical processors, using up to 24 threads\n",
      "\n",
      "Optimize a model with 9763 rows, 18517 columns and 14987 nonzeros\n",
      "Model fingerprint: 0x990764d4\n",
      "Variable types: 17936 continuous, 581 integer (581 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-03, 5e+04]\n",
      "  Objective range  [1e+00, 2e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [2e+04, 5e+04]\n",
      "Found heuristic solution: objective 2880000.0000\n",
      "Presolve removed 9526 rows and 18367 columns\n",
      "Presolve time: 0.01s\n",
      "Presolved: 237 rows, 150 columns, 542 nonzeros\n",
      "Variable types: 103 continuous, 47 integer (47 binary)\n",
      "\n",
      "Root relaxation: objective 2.561040e+06, 77 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0    2561040.0000 2561040.00  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (77 simplex iterations) in 0.02 seconds (0.02 work units)\n",
      "Thread count was 24 (of 24 available processors)\n",
      "\n",
      "Solution count 2: 2.56104e+06 2.88e+06 \n",
      "\n",
      "Optimal solution found (tolerance 5.00e-03)\n",
      "Best objective 2.561040000000e+06, best bound 2.561040000000e+06, gap 0.0000%\n",
      "WARNING: Cannot get duals for MIP.\n",
      "######################## Finished solving cap4_1 cap7_1 trans12_2 res1_1 res6_1 trans25_nd cap2_nd (8 of 4096) ########################\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[32], line 27\u001B[0m\n\u001B[0;32m     11\u001B[0m model_PI\u001B[38;5;241m.\u001B[39mconstraint_nameplate_production_varying_capacity \u001B[38;5;241m=\u001B[39m make_constraint(instance\u001B[38;5;241m=\u001B[39mmodel_PI,\n\u001B[0;32m     12\u001B[0m     type_cons\u001B[38;5;241m=\u001B[39mCons\u001B[38;5;241m.\u001B[39mX_LEQ_BY, variable_x\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mP\u001B[39m\u001B[38;5;124m'\u001B[39m, location_set\u001B[38;5;241m=\u001B[39mmodel_PI\u001B[38;5;241m.\u001B[39mlocations, component_set\u001B[38;5;241m=\u001B[39mmodel_PI\u001B[38;5;241m.\u001B[39mprocesses_varying_capacity,\n\u001B[0;32m     13\u001B[0m     loc_comp_dict\u001B[38;5;241m=\u001B[39mscen_PI\u001B[38;5;241m.\u001B[39mlocation_process_dict, b_factor\u001B[38;5;241m=\u001B[39mscen_PI\u001B[38;5;241m.\u001B[39mcapacity_factor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     17\u001B[0m                                                                             variable_y\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCap_P\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     18\u001B[0m                                                                             label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrestricts production to varying nameplate capacity\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     20\u001B[0m model_PI\u001B[38;5;241m.\u001B[39mconstraint_resource_consumption_varying \u001B[38;5;241m=\u001B[39m make_constraint(\n\u001B[0;32m     21\u001B[0m     instance\u001B[38;5;241m=\u001B[39mmodel_PI, type_cons\u001B[38;5;241m=\u001B[39mCons\u001B[38;5;241m.\u001B[39mX_LEQ_B, variable_x\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC\u001B[39m\u001B[38;5;124m'\u001B[39m, location_set\u001B[38;5;241m=\u001B[39mmodel_PI\u001B[38;5;241m.\u001B[39mlocations,\n\u001B[0;32m     22\u001B[0m     component_set\u001B[38;5;241m=\u001B[39mmodel_PI\u001B[38;5;241m.\u001B[39mresources_varying_availability, b_max\u001B[38;5;241m=\u001B[39mscen_PI\u001B[38;5;241m.\u001B[39mcons_max,\n\u001B[0;32m     23\u001B[0m     loc_comp_dict\u001B[38;5;241m=\u001B[39mscen_PI\u001B[38;5;241m.\u001B[39mlocation_resource_dict, b_factor\u001B[38;5;241m=\u001B[39mscen_PI\u001B[38;5;241m.\u001B[39mavailability_factor,\n\u001B[0;32m     24\u001B[0m     x_scale_level\u001B[38;5;241m=\u001B[39mscen_PI\u001B[38;5;241m.\u001B[39mscheduling_scale_level, b_scale_level\u001B[38;5;241m=\u001B[39mscen_PI\u001B[38;5;241m.\u001B[39mavailability_scale_level,\n\u001B[0;32m     25\u001B[0m     label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrestricts resource consumption to varying availablity\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 27\u001B[0m \u001B[43mconstraint_export\u001B[49m\u001B[43m(\u001B[49m\u001B[43minstance\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_PI\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduling_scale_level\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscen_PI\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscheduling_scale_level\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mnetwork_scale_level\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscen_PI\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnetwork_scale_level\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     29\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mlocation_transport_resource_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscen_PI\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlocation_transport_resource_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     30\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mtransport_capacity_factor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscen_PI\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransport_capacity_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     31\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mtransport_capacity_scale_level\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscen_PI\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransport_capacity_scale_level\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     33\u001B[0m results_PI \u001B[38;5;241m=\u001B[39m solve(scenario\u001B[38;5;241m=\u001B[39mscen_PI, instance\u001B[38;5;241m=\u001B[39mmodel_PI, solver\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgurobi\u001B[39m\u001B[38;5;124m'\u001B[39m, name\u001B[38;5;241m=\u001B[39mscen_name,\n\u001B[0;32m     34\u001B[0m                    solver_options\u001B[38;5;241m=\u001B[39msolver_options)\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m######################## Finished solving \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mscen_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcounter\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(load_scenario_names)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) ########################\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\energiapy\\src\\energiapy\\model\\constraints\\transport.py:153\u001B[0m, in \u001B[0;36mconstraint_export\u001B[1;34m(instance, scheduling_scale_level, network_scale_level, location_transport_resource_dict, transport_capacity_factor, transport_capacity_scale_level)\u001B[0m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;66;03m# in instance.resources_trans if resource_\u001B[39;00m\n\u001B[0;32m    150\u001B[0m instance\u001B[38;5;241m.\u001B[39mconstraint_export \u001B[38;5;241m=\u001B[39m Constraint(instance\u001B[38;5;241m.\u001B[39msources, instance\u001B[38;5;241m.\u001B[39msinks,\n\u001B[0;32m    151\u001B[0m                                         instance\u001B[38;5;241m.\u001B[39mtransports, \u001B[38;5;241m*\u001B[39mscales, rule\u001B[38;5;241m=\u001B[39mexport_rule,\n\u001B[0;32m    152\u001B[0m                                         doc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcapacity bound export of resource from source to sink\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 153\u001B[0m \u001B[43mconstraint_latex_render\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexport_rule\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m instance\u001B[38;5;241m.\u001B[39mconstraint_export\n",
      "File \u001B[1;32m~\\PycharmProjects\\energiapy\\src\\energiapy\\utils\\latex_utils.py:92\u001B[0m, in \u001B[0;36mconstraint_latex_render\u001B[1;34m(constraint_rule, latex_alias_dict)\u001B[0m\n\u001B[0;32m     89\u001B[0m list_\u001B[38;5;241m.\u001B[39mreverse()\n\u001B[0;32m     91\u001B[0m dict_ \u001B[38;5;241m=\u001B[39m {i: unsorted_dict_[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m list_}\n\u001B[1;32m---> 92\u001B[0m str_ \u001B[38;5;241m=\u001B[39m \u001B[43minspect\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetsource\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconstraint_rule\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39msplit(\n\u001B[0;32m     93\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreturn \u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     94\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m dict_\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m     95\u001B[0m     str_ \u001B[38;5;241m=\u001B[39m str_\u001B[38;5;241m.\u001B[39mreplace(key, dict_[key])\n",
      "File \u001B[1;32m~\\.conda\\envs\\venv_energiapy\\lib\\inspect.py:1139\u001B[0m, in \u001B[0;36mgetsource\u001B[1;34m(object)\u001B[0m\n\u001B[0;32m   1133\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgetsource\u001B[39m(\u001B[38;5;28mobject\u001B[39m):\n\u001B[0;32m   1134\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return the text of the source code for an object.\u001B[39;00m\n\u001B[0;32m   1135\u001B[0m \n\u001B[0;32m   1136\u001B[0m \u001B[38;5;124;03m    The argument may be a module, class, method, function, traceback, frame,\u001B[39;00m\n\u001B[0;32m   1137\u001B[0m \u001B[38;5;124;03m    or code object.  The source code is returned as a single string.  An\u001B[39;00m\n\u001B[0;32m   1138\u001B[0m \u001B[38;5;124;03m    OSError is raised if the source code cannot be retrieved.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1139\u001B[0m     lines, lnum \u001B[38;5;241m=\u001B[39m \u001B[43mgetsourcelines\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mobject\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(lines)\n",
      "File \u001B[1;32m~\\.conda\\envs\\venv_energiapy\\lib\\inspect.py:1121\u001B[0m, in \u001B[0;36mgetsourcelines\u001B[1;34m(object)\u001B[0m\n\u001B[0;32m   1113\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Return a list of source lines and starting line number for an object.\u001B[39;00m\n\u001B[0;32m   1114\u001B[0m \n\u001B[0;32m   1115\u001B[0m \u001B[38;5;124;03mThe argument may be a module, class, method, function, traceback, frame,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1118\u001B[0m \u001B[38;5;124;03moriginal source file the first line of code was found.  An OSError is\u001B[39;00m\n\u001B[0;32m   1119\u001B[0m \u001B[38;5;124;03mraised if the source code cannot be retrieved.\"\"\"\u001B[39;00m\n\u001B[0;32m   1120\u001B[0m \u001B[38;5;28mobject\u001B[39m \u001B[38;5;241m=\u001B[39m unwrap(\u001B[38;5;28mobject\u001B[39m)\n\u001B[1;32m-> 1121\u001B[0m lines, lnum \u001B[38;5;241m=\u001B[39m \u001B[43mfindsource\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mobject\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1123\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m istraceback(\u001B[38;5;28mobject\u001B[39m):\n\u001B[0;32m   1124\u001B[0m     \u001B[38;5;28mobject\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mobject\u001B[39m\u001B[38;5;241m.\u001B[39mtb_frame\n",
      "File \u001B[1;32m~\\.conda\\envs\\venv_energiapy\\lib\\inspect.py:940\u001B[0m, in \u001B[0;36mfindsource\u001B[1;34m(object)\u001B[0m\n\u001B[0;32m    932\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfindsource\u001B[39m(\u001B[38;5;28mobject\u001B[39m):\n\u001B[0;32m    933\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return the entire source file and starting line number for an object.\u001B[39;00m\n\u001B[0;32m    934\u001B[0m \n\u001B[0;32m    935\u001B[0m \u001B[38;5;124;03m    The argument may be a module, class, method, function, traceback, frame,\u001B[39;00m\n\u001B[0;32m    936\u001B[0m \u001B[38;5;124;03m    or code object.  The source code is returned as a list of all the lines\u001B[39;00m\n\u001B[0;32m    937\u001B[0m \u001B[38;5;124;03m    in the file and the line number indexes a line in that list.  An OSError\u001B[39;00m\n\u001B[0;32m    938\u001B[0m \u001B[38;5;124;03m    is raised if the source code cannot be retrieved.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 940\u001B[0m     file \u001B[38;5;241m=\u001B[39m \u001B[43mgetsourcefile\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mobject\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    941\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m file:\n\u001B[0;32m    942\u001B[0m         \u001B[38;5;66;03m# Invalidate cache if needed.\u001B[39;00m\n\u001B[0;32m    943\u001B[0m         linecache\u001B[38;5;241m.\u001B[39mcheckcache(file)\n",
      "File \u001B[1;32m~\\.conda\\envs\\venv_energiapy\\lib\\inspect.py:826\u001B[0m, in \u001B[0;36mgetsourcefile\u001B[1;34m(object)\u001B[0m\n\u001B[0;32m    823\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28many\u001B[39m(filename\u001B[38;5;241m.\u001B[39mendswith(s) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m\n\u001B[0;32m    824\u001B[0m              importlib\u001B[38;5;241m.\u001B[39mmachinery\u001B[38;5;241m.\u001B[39mEXTENSION_SUFFIXES):\n\u001B[0;32m    825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 826\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexists\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m    827\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m filename\n\u001B[0;32m    828\u001B[0m \u001B[38;5;66;03m# only return a non-existent filename if the module has a PEP 302 loader\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\venv_energiapy\\lib\\genericpath.py:19\u001B[0m, in \u001B[0;36mexists\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 19\u001B[0m     \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mOSError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Deterministic Scenarios for Perfect Information\n",
    "for scen_name in load_scenario_names:\n",
    "    scen_PI = build_model(scen_df=load_scenario_dict[scen_name]['factor'])\n",
    "    counter+=1\n",
    "    # Delete process capacity factors, resource availability factors, transport capacity factors\n",
    "    model_PI.del_component('constraint_nameplate_production_varying_capacity')\n",
    "    model_PI.del_component('constraint_resource_consumption_varying')\n",
    "    model_PI.del_component('constraint_export')\n",
    "\n",
    "    # Add the constraints back for this particular scenario\n",
    "    model_PI.constraint_nameplate_production_varying_capacity = make_constraint(instance=model_PI,\n",
    "        type_cons=Cons.X_LEQ_BY, variable_x='P', location_set=model_PI.locations, component_set=model_PI.processes_varying_capacity,\n",
    "        loc_comp_dict=scen_PI.location_process_dict, b_factor=scen_PI.capacity_factor,\n",
    "                                                                                x_scale_level=scen_PI.scheduling_scale_level,\n",
    "                                                                                b_scale_level=scen_PI.capacity_scale_level,\n",
    "                                                                                y_scale_level=scen_PI.network_scale_level,\n",
    "                                                                                variable_y='Cap_P',\n",
    "                                                                                label='restricts production to varying nameplate capacity')\n",
    "\n",
    "    model_PI.constraint_resource_consumption_varying = make_constraint(\n",
    "        instance=model_PI, type_cons=Cons.X_LEQ_B, variable_x='C', location_set=model_PI.locations,\n",
    "        component_set=model_PI.resources_varying_availability, b_max=scen_PI.cons_max,\n",
    "        loc_comp_dict=scen_PI.location_resource_dict, b_factor=scen_PI.availability_factor,\n",
    "        x_scale_level=scen_PI.scheduling_scale_level, b_scale_level=scen_PI.availability_scale_level,\n",
    "        label='restricts resource consumption to varying availablity')\n",
    "\n",
    "    constraint_export(instance=model_PI, scheduling_scale_level=scen_PI.scheduling_scale_level,\n",
    "                      network_scale_level=scen_PI.network_scale_level,\n",
    "                      location_transport_resource_dict=scen_PI.location_transport_resource_dict,\n",
    "                      transport_capacity_factor=scen_PI.transport_capacity_factor,\n",
    "                      transport_capacity_scale_level=scen_PI.transport_capacity_scale_level)\n",
    "\n",
    "    results_PI = solve(scenario=scen_PI, instance=model_PI, solver='gurobi', name=scen_name,\n",
    "                       solver_options=solver_options)\n",
    "\n",
    "    print(f'######################## Finished solving {scen_name} ({counter} of {len(load_scenario_names)}) ########################')\n",
    "\n",
    "    model_vars = model_PI.component_map(ctype=Var)\n",
    "    vars_dict = {i: model_vars[i].extract_values() for i in model_vars.keys()}\n",
    "    obj_dict = {'objective': pyoval(model_PI.objective_cost_w_demand_penalty)}\n",
    "\n",
    "    PI_output_dict[scen_name] = {**vars_dict, **obj_dict}\n",
    "\n",
    "    with open(fr\"{PI_pkl_folder}\\{scen_name}_{len(load_scenario_names)}.pkl\", 'wb') as file:\n",
    "        pickle.dump(PI_output_dict[scen_name], file)\n",
    "\n",
    "    exCost_PI += pyoval(model_PI.objective_cost_w_demand_penalty) * load_scenario_dict[scen_name]['prob']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:37:12.912875Z",
     "start_time": "2024-11-05T16:37:06.738504Z"
    }
   },
   "id": "a5ca727b06cf56e4",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A child process terminated abruptly, the process pool is not usable anymore",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mBrokenProcessPool\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 71\u001B[0m\n\u001B[0;32m     68\u001B[0m exCost_PI_total \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     70\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ProcessPoolExecutor() \u001B[38;5;28;01mas\u001B[39;00m executor:\n\u001B[1;32m---> 71\u001B[0m     futures \u001B[38;5;241m=\u001B[39m {executor\u001B[38;5;241m.\u001B[39msubmit(process_scenario, scen_name): scen_name \u001B[38;5;28;01mfor\u001B[39;00m scen_name \u001B[38;5;129;01min\u001B[39;00m load_scenario_names}\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m future \u001B[38;5;129;01min\u001B[39;00m as_completed(futures):\n\u001B[0;32m     73\u001B[0m         scen_name, output_dict, exCost_PI \u001B[38;5;241m=\u001B[39m future\u001B[38;5;241m.\u001B[39mresult()\n",
      "Cell \u001B[1;32mIn[19], line 71\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     68\u001B[0m exCost_PI_total \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     70\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ProcessPoolExecutor() \u001B[38;5;28;01mas\u001B[39;00m executor:\n\u001B[1;32m---> 71\u001B[0m     futures \u001B[38;5;241m=\u001B[39m {\u001B[43mexecutor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubmit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_scenario\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscen_name\u001B[49m\u001B[43m)\u001B[49m: scen_name \u001B[38;5;28;01mfor\u001B[39;00m scen_name \u001B[38;5;129;01min\u001B[39;00m load_scenario_names}\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m future \u001B[38;5;129;01min\u001B[39;00m as_completed(futures):\n\u001B[0;32m     73\u001B[0m         scen_name, output_dict, exCost_PI \u001B[38;5;241m=\u001B[39m future\u001B[38;5;241m.\u001B[39mresult()\n",
      "File \u001B[1;32m~\\.conda\\envs\\venv_energiapy\\lib\\concurrent\\futures\\process.py:720\u001B[0m, in \u001B[0;36mProcessPoolExecutor.submit\u001B[1;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m    718\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown_lock:\n\u001B[0;32m    719\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_broken:\n\u001B[1;32m--> 720\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m BrokenProcessPool(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_broken)\n\u001B[0;32m    721\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown_thread:\n\u001B[0;32m    722\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcannot schedule new futures after shutdown\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mBrokenProcessPool\u001B[0m: A child process terminated abruptly, the process pool is not usable anymore"
     ]
    }
   ],
   "source": [
    "# from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "# import pickle\n",
    "# import os\n",
    "# \n",
    "# # Define the main function to handle each scenario\n",
    "# def process_scenario(scen_name):\n",
    "#     try:\n",
    "#         scen_PI = build_model(scen_df=load_scenario_dict[scen_name]['factor'])\n",
    "#         \n",
    "#         # Modify model as needed\n",
    "#         model_PI.del_component('constraint_nameplate_production_varying_capacity')\n",
    "#         model_PI.del_component('constraint_resource_consumption_varying')\n",
    "#         model_PI.del_component('constraint_export')\n",
    "#     \n",
    "#         # Add constraints\n",
    "#         model_PI.constraint_nameplate_production_varying_capacity = make_constraint(\n",
    "#             instance=model_PI, type_cons=Cons.X_LEQ_BY, variable_x='P', \n",
    "#             location_set=model_PI.locations, component_set=model_PI.processes_varying_capacity,\n",
    "#             loc_comp_dict=scen_PI.location_process_dict, b_factor=scen_PI.capacity_factor,\n",
    "#             x_scale_level=scen_PI.scheduling_scale_level, b_scale_level=scen_PI.capacity_scale_level,\n",
    "#             y_scale_level=scen_PI.network_scale_level, variable_y='Cap_P',\n",
    "#             label='restricts production to varying nameplate capacity'\n",
    "#         )\n",
    "#     \n",
    "#         model_PI.constraint_resource_consumption_varying = make_constraint(\n",
    "#             instance=model_PI, type_cons=Cons.X_LEQ_B, variable_x='C', \n",
    "#             location_set=model_PI.locations, component_set=model_PI.resources_varying_availability,\n",
    "#             b_max=scen_PI.cons_max, loc_comp_dict=scen_PI.location_resource_dict, \n",
    "#             b_factor=scen_PI.availability_factor, x_scale_level=scen_PI.scheduling_scale_level, \n",
    "#             b_scale_level=scen_PI.availability_scale_level,\n",
    "#             label='restricts resource consumption to varying availability'\n",
    "#         )\n",
    "#     \n",
    "#         constraint_export(\n",
    "#             instance=model_PI, scheduling_scale_level=scen_PI.scheduling_scale_level,\n",
    "#             network_scale_level=scen_PI.network_scale_level,\n",
    "#             location_transport_resource_dict=scen_PI.location_transport_resource_dict,\n",
    "#             transport_capacity_factor=scen_PI.transport_capacity_factor,\n",
    "#             transport_capacity_scale_level=scen_PI.transport_capacity_scale_level\n",
    "#         )\n",
    "#     \n",
    "#         # Solve the model\n",
    "#         results_PI = solve(scenario=scen_PI, instance=model_PI, solver='gurobi', name=scen_name, solver_options=solver_options)\n",
    "#     \n",
    "#         # Extract results\n",
    "#         model_vars = model_PI.component_map(ctype=Var)\n",
    "#         vars_dict = {i: model_vars[i].extract_values() for i in model_vars.keys()}\n",
    "#         obj_dict = {'objective': pyoval(model_PI.objective_cost_w_demand_penalty)}\n",
    "#     \n",
    "#         # Save results\n",
    "#         output_dict = {**vars_dict, **obj_dict}\n",
    "#         output_file = os.path.join(PI_pkl_folder, f\"{scen_name}_{len(load_scenario_names)}.pkl\")\n",
    "#         with open(output_file, 'wb') as file:\n",
    "#             pickle.dump(output_dict, file)\n",
    "#     \n",
    "#         exCost_PI = pyoval(model_PI.objective_cost_w_demand_penalty) * load_scenario_dict[scen_name]['prob']\n",
    "#     \n",
    "#         print(f'######################## Finished solving {scen_name} ########################')\n",
    "#         \n",
    "#         return scen_name, output_dict, exCost_PI\n",
    "#     \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {scen_name}: {e}\")\n",
    "#         return scen_name, None, None\n",
    "#         \n",
    "# # Run in parallel\n",
    "# PI_output_dict = {}\n",
    "# exCost_PI_total = 0\n",
    "# \n",
    "# with ProcessPoolExecutor() as executor:\n",
    "#     futures = {executor.submit(process_scenario, scen_name): scen_name for scen_name in load_scenario_names}\n",
    "#     for future in as_completed(futures):\n",
    "#         scen_name, output_dict, exCost_PI = future.result()\n",
    "#         PI_output_dict[scen_name] = output_dict\n",
    "#         exCost_PI_total += exCost_PI\n",
    "# \n",
    "# print(\"Total expected cost:\", exCost_PI_total)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:32:25.740806Z",
     "start_time": "2024-11-05T16:32:25.548046Z"
    }
   },
   "id": "3337e0c72f383083",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f\"Total Expected Cost considering perfect information: {exCost_PI:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "102c401fd8bf812f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(fr\"{PI_pkl_folder}\\exCost_{len(load_scenario_names)}_PI_HP.pkl\", 'wb') as file:\n",
    "    pickle.dump(exCost_PI, file)\n",
    "\n",
    "with open(fr\"{PI_pkl_folder}\\exCost_{len(load_scenario_names)}_PI_HP.pkl\", 'rb') as file:\n",
    "    exCost_PI_load = pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdbd8eff91bc5872",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "exCost_PI_load"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "318b4fecace53495",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "load_PI_dict = dict()\n",
    "for file in os.listdir(PI_pkl_folder):\n",
    "    if file.endswith('.pkl') and not file.endswith('_HP.pkl'):\n",
    "        full_file_path = os.path.join(PI_pkl_folder, file)\n",
    "        \n",
    "        with open(full_file_path, 'rb') as f:\n",
    "            load_PI_dict[file.removesuffix(f'_{len(load_scenario_names)}.pkl')] = pickle.load(f)\n",
    "            \n",
    "# len(PI_load_dict)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3eafe3f4d8bec798",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def find_common_substring(lst):\n",
    "    \n",
    "    # Split each string into substrings and create sets\n",
    "    substring_sets = [set(item.split()) for item in lst]\n",
    "    \n",
    "    # Find the intersection of all sets to get common substrings\n",
    "    common_substrings = reduce(lambda a, b: a & b, substring_sets)\n",
    "    \n",
    "    return common_substrings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fa19a5f8b601b92",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def explore_dict(d, level=0):\n",
    "    indent = \"    \" * level  # Indentation for better readability\n",
    "    for key, value in d.items():\n",
    "        print(f\"{indent}Key: {key}, Type of value: {type(value)}\")\n",
    "        if isinstance(value, dict):  # Recursively explore dictionaries\n",
    "            explore_dict(value, level + 1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e7732d502f75140",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def filter_scenarios():\n",
    "    # Initialize the result dictionary\n",
    "    result = {}\n",
    "    \n",
    "    # Step 1: Iterate over the original data\n",
    "    for scenario, data in load_PI_dict.items():\n",
    "        x_val = data['Demand_penalty_network'][('com1_sold',0)]\n",
    "        obj_val = data['objective'] - data['Demand_penalty_network'][('com1_sold',0)]\n",
    "        \n",
    "        # Step 2: Check if x_val is already a key in the result\n",
    "        if x_val not in result:\n",
    "            result[x_val] = {'count': 0, 'objectives': {}}\n",
    "        \n",
    "        # Increment the count for the x value\n",
    "        result[x_val]['count'] += 1\n",
    "        \n",
    "        # Step 3: Check if obj_val is already a key under the 'objectives' for this x\n",
    "        if obj_val not in result[x_val]['objectives']:\n",
    "            result[x_val]['objectives'][obj_val] = {'scenarios': [], 'count': 0}\n",
    "        \n",
    "        # Add the scenario to the list and increment the count for the objective\n",
    "        result[x_val]['objectives'][obj_val]['scenarios'].append(scenario)\n",
    "        result[x_val]['objectives'][obj_val]['count'] += 1\n",
    "    \n",
    "    # Step 4: Sort the result by x values (in decreasing order) and objectives (in decreasing order)\n",
    "    sorted_result = {\n",
    "        x_val: {\n",
    "            'count': result[x_val]['count'],\n",
    "            'count_obj': len(result[x_val]['objectives']),\n",
    "            'objectives': {\n",
    "                obj_val: result[x_val]['objectives'][obj_val]\n",
    "                for obj_val in sorted(result[x_val]['objectives'].keys(), reverse=True)\n",
    "            }\n",
    "        }\n",
    "        for x_val in sorted(result.keys(), reverse=True)\n",
    "    }\n",
    "    \n",
    "    return sorted_result"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3c741f096b23343",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# def pick_scenarios(sorted_dict, n):\n",
    "#     selected_scenarios = {}\n",
    "#     total_selected = 0\n",
    "# \n",
    "#     # Iterate over sorted x values\n",
    "#     for x_val, x_data in sorted_dict.items():\n",
    "#         if total_selected >= n:\n",
    "#             break  # Stop if we have selected enough scenarios\n",
    "# \n",
    "#         # Iterate over sorted objective values within each x\n",
    "#         for obj_val, obj_data in x_data['objectives'].items():\n",
    "#             if total_selected >= n:\n",
    "#                 break  # Stop if we have selected enough scenarios\n",
    "# \n",
    "#             # Randomly select from the list of scenarios if there are multiple\n",
    "#             scenarios_to_choose = obj_data['scenarios']\n",
    "#             num_to_pick = min(len(scenarios_to_choose), 1)\n",
    "#             chosen_scenarios = random.sample(scenarios_to_choose, num_to_pick)\n",
    "# \n",
    "#             # Add the selected scenarios to the result\n",
    "#             if x_val not in selected_scenarios:\n",
    "#                 selected_scenarios[x_val] = {}\n",
    "#             \n",
    "#             if obj_val not in selected_scenarios[x_val]:\n",
    "#                 selected_scenarios[x_val][obj_val] = {}\n",
    "# \n",
    "#             for scenario in chosen_scenarios:\n",
    "#                 selected_scenarios[x_val][obj_val][scenario] = {\n",
    "#                     'prob': load_scenario_dict[scenario]['prob'],\n",
    "#                     'factor': load_scenario_dict[scenario]['factor'].copy()\n",
    "#                 }\n",
    "#                 total_selected += 1\n",
    "# \n",
    "#                 if total_selected >= n:\n",
    "#                     break  # Stop if we have selected enough scenarios\n",
    "# \n",
    "#     return selected_scenarios"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e93686de5aa3e1e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def pick_scenario_with_highest_probability(scenario_list, scenario_dict):\n",
    "    # Filter scenarios in the list that are present in the dictionary\n",
    "    valid_scenarios = [(s, scenario_dict[s]['prob']) for s in scenario_list if s in scenario_dict]\n",
    "\n",
    "    # Find the maximum probability among valid scenarios\n",
    "    max_prob = min(valid_scenarios, key=lambda x: x[1], default=(None, -1))[1]\n",
    "\n",
    "    # Collect scenarios with the maximum probability\n",
    "    highest_prob_scenarios = [s for s, p in valid_scenarios if p == max_prob]\n",
    "\n",
    "    return random.choice(highest_prob_scenarios) if highest_prob_scenarios else None"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b79f7c85e89611b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def pick_scenarios(sorted_dict, n):\n",
    "    selected_scenarios = {}\n",
    "    total_selected = 0\n",
    "\n",
    "    # Iterate over sorted x values\n",
    "    for x_val, x_data in sorted_dict.items():\n",
    "        if total_selected >= n:\n",
    "            break  # Stop if we have selected enough scenarios\n",
    "\n",
    "        # Iterate over sorted objective values within each x\n",
    "        for obj_val, obj_data in x_data['objectives'].items():\n",
    "            if total_selected >= n:\n",
    "                break  # Stop if we have selected enough scenarios\n",
    "\n",
    "            # Get the list of scenarios for the current objective\n",
    "            scenarios_to_choose = obj_data['scenarios']\n",
    "\n",
    "            # Choose the scenario with the highest probability using the previous logic\n",
    "            chosen_scenario = pick_scenario_with_highest_probability(scenarios_to_choose, load_scenario_dict)\n",
    "\n",
    "            # Add the selected scenario to the result\n",
    "            if chosen_scenario:\n",
    "                if x_val not in selected_scenarios:\n",
    "                    selected_scenarios[x_val] = {}\n",
    "\n",
    "                if obj_val not in selected_scenarios[x_val]:\n",
    "                    selected_scenarios[x_val][obj_val] = {}\n",
    "\n",
    "                selected_scenarios[x_val][obj_val][chosen_scenario] = {\n",
    "                    'prob': load_scenario_dict[chosen_scenario]['prob'],\n",
    "                    'factor': load_scenario_dict[chosen_scenario]['factor'].copy()\n",
    "                }\n",
    "                total_selected += 1\n",
    "\n",
    "            if total_selected >= n:\n",
    "                break  # Stop if we have selected enough scenarios\n",
    "\n",
    "    return selected_scenarios\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e55254cb9590d42c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def sum_probabilities(d):\n",
    "    total_prob = 0\n",
    "    \n",
    "    def recursive_sum(current_dict):\n",
    "        nonlocal total_prob\n",
    "        for key, value in current_dict.items():\n",
    "            if isinstance(value, dict):\n",
    "                # Recursively traverse if it's still a dictionary\n",
    "                recursive_sum(value)\n",
    "            elif key == 'prob':\n",
    "                # Add the probability value\n",
    "                total_prob += value\n",
    "\n",
    "    recursive_sum(d)\n",
    "    return total_prob"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8456a22fc5a5fc95",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(fr\"sorted_result_{len(load_scenario_names)}_HP.pkl\", 'rb') as file:\n",
    "    load_sorted_result = pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "901c8c88b186efca",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Example data\n",
    "n = len(list(load_sorted_result.keys()))\n",
    "list1 = [i for i in range(1,n+1)]  # X-axis labels\n",
    "list2 = [load_sorted_result[key]['count'] for key, data in load_sorted_result.items()]\n",
    "\n",
    "# Create a bar chart\n",
    "plt.bar(list1, list2)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Demand Penalty Values')\n",
    "plt.ylabel('Number of Scenarios')\n",
    "plt.title('Demand Penalty Value Histogram')\n",
    "\n",
    "for i, value in enumerate(list2):\n",
    "    plt.text(i+1, value, str(value), ha='center', va='bottom')\n",
    "\n",
    "# Rotate the x-axis labels if they are too long or overlapping\n",
    "plt.xticks()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f55df7365e9a223",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sum(data['count_obj'] for key, data in load_sorted_result.items())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9774782444817311",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "penalty_key = list(load_sorted_result.keys())[0]\n",
    "n = 10\n",
    "obj_keys = [list(load_sorted_result[penalty_key]['objectives'].keys())[i] for i in range(n)]\n",
    "list1 = [i for i in range(1,n+1)]  # X-axis labels\n",
    "list2 = [load_sorted_result[penalty_key]['objectives'][obj_keys[i]]['count'] for i in range(n)]\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "# Create a bar chart\n",
    "plt.bar(list1, list2)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Scenarios')\n",
    "plt.ylabel('#')\n",
    "plt.title('Number of occurences')\n",
    "\n",
    "for i, value in enumerate(list2):\n",
    "    plt.text(i+1, value+0.25, str(value), ha='center', va='bottom', fontsize=13)\n",
    "\n",
    "# Rotate the x-axis labels if they are too long or overlapping\n",
    "# plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3dd59ea1805d2c79",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "error"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e55445041546ef70"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STOP PI EXECUTION HERE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29dde4d8795030e4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RUN FROM HERE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4c94a3e0594f194"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "scenarios_to_select = 75\n",
    "random.seed(2)\n",
    "selected_scenarios = pick_scenarios(load_sorted_result, scenarios_to_select)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ab4d75bafd846b3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "norm_factor = sum_probabilities(selected_scenarios)\n",
    "selected_scenario_dict = {\n",
    "    k: {\n",
    "        'prob': scenario_data['prob'] / norm_factor,\n",
    "        'factor': scenario_data['factor'].copy()\n",
    "    }\n",
    "    for i, inner_dict in selected_scenarios.items()\n",
    "    for j, sub_dict in inner_dict.items()\n",
    "    for k, scenario_data in sub_dict.items()\n",
    "}\n",
    "            \n",
    "selected_scenario_names = list(selected_scenario_dict.keys())\n",
    "print(f'Number of scenarios considered: {len(selected_scenario_names)}')\n",
    "prob_select = sum_probabilities(selected_scenario_dict)\n",
    "print(f'Sum of probabilities of selected scenarios: {prob_select:.6f}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d67d2ad58d19fc55",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STOCHASTIC PROBLEM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5f94d9374b84ca1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "options = {\"solver\": \"gurobi\"}\n",
    "scenario_creator_kwargs = {'scenario_dict': selected_scenario_dict}\n",
    "ef_UI = ExtensiveForm(options, selected_scenario_names, scenario_creator, scenario_creator_kwargs=scenario_creator_kwargs)\n",
    "results = ef_UI.solve_extensive_form(solver_options=solver_options)\n",
    "end_time = time.time()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29824de25706215e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "exCost_UI = ef_UI.get_objective_value()\n",
    "# exCost_UI\n",
    "\n",
    "with open(fr\"{FD_pkl_folder}\\{len(selected_scenario_names)}\\exCost_{len(selected_scenario_names)}_UI_HP.pkl\", 'wb') as file:\n",
    "    pickle.dump(exCost_UI, file)\n",
    "    \n",
    "with open(fr\"{FD_pkl_folder}\\{len(selected_scenario_names)}\\exCost_{len(selected_scenario_names)}_UI_HP.pkl\", 'rb') as file:\n",
    "    load_exCost_UI = pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "deac04f3c04fe83",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# exCost_UI = 2133007.7662\n",
    "# EVPI = exCost_UI - exCost_PI_load\n",
    "# p_inc = EVPI * 100 / exCost_PI_load\n",
    "# p_dec = EVPI * 100 / exCost_UI"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c05fada84f0eb05",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ssoln_dict = ef_UI.get_root_solution()\n",
    "with open(fr\"{FD_pkl_folder}\\{len(selected_scenario_names)}\\ssoln_{len(selected_scenario_names)}_UI_HP.pkl\", 'wb') as file:\n",
    "    pickle.dump(ssoln_dict, file)\n",
    "# \n",
    "# print(f\"Total Expected Cost considering perfect information: {exCost_PI_load:.4f}\")\n",
    "# print(f\"Total Expected Cost considering disruptions (stochastic solution): {exCost_UI:.4f}\")\n",
    "# print(f\"Expected Value of Perfect Information: {EVPI:.4f}\")\n",
    "# print(f\"Percentage Increase in Cost: {p_inc:.4f}%%\")\n",
    "# print(f\"Percentage Decrease in Cost: {p_dec:.4f}%%\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "305759d4164e53bb",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# FIXED DESIGN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afe4b4a492918e59"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "exCost_FD = 0\n",
    "exRes_FD = 0\n",
    "results_FD = dict()\n",
    "scen_FD, model_FD = build_model()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53dea46b595cd069",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def fix_variables(model1: ConcreteModel, model2:ConcreteModel, scen_name:str):\n",
    "    # vars_to_fix = ['X_P', 'Cap_P', 'X_S', 'Cap_S', 'X_F', 'Cap_F']\n",
    "    model = getattr(model1, scen_name)\n",
    "    def fix(var1, var2):\n",
    "        for i in list(var1.keys()):\n",
    "            if var1[i].value is None:\n",
    "                continue\n",
    "            else:\n",
    "                var2[i].fixed = True\n",
    "                var2[i] = var1[i].value\n",
    "            # var2[i].pprint()\n",
    "            \n",
    "    fix(model.X_P, model2.X_P)\n",
    "    fix(model.Cap_P, model2.Cap_P)\n",
    "    fix(model.X_S, model2.X_S)\n",
    "    fix(model.Cap_S, model2.Cap_S)\n",
    "    fix(model.X_F, model2.X_F)\n",
    "    fix(model.Cap_F, model2.Cap_F)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4198bb810529bc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Deterministic Scenarios for Fixed Design\n",
    "counter = 0\n",
    "FD_output_dict=dict()\n",
    "fix_variables(model1=ef_UI.ef, model2=model_FD, scen_name=selected_scenario_names[0])\n",
    "\n",
    "for scen_name in load_scenario_names:\n",
    "    scen_FD = build_model(scen_df=load_scenario_dict[scen_name]['factor'])\n",
    "    counter+=1\n",
    "    # Delete process capacity factors, resource availability factors, transport capacity factors\n",
    "    model_FD.del_component('constraint_nameplate_production_varying_capacity')\n",
    "    model_FD.del_component('constraint_resource_consumption_varying')\n",
    "    model_FD.del_component('constraint_export')\n",
    "\n",
    "    # Add the constraints back for this particular scenario\n",
    "    model_FD.constraint_nameplate_production_varying_capacity = make_constraint(instance=model_FD,\n",
    "        type_cons=Cons.X_LEQ_BY, variable_x='P', location_set=model_FD.locations, component_set=model_FD.processes_varying_capacity,\n",
    "        loc_comp_dict=scen_FD.location_process_dict, b_factor=scen_FD.capacity_factor,\n",
    "                                                                                x_scale_level=scen_FD.scheduling_scale_level,\n",
    "                                                                                b_scale_level=scen_FD.capacity_scale_level,\n",
    "                                                                                y_scale_level=scen_FD.network_scale_level,\n",
    "                                                                                variable_y='Cap_P',\n",
    "                                                                                label='restricts production to varying nameplate capacity')\n",
    "\n",
    "    model_FD.constraint_resource_consumption_varying = make_constraint(\n",
    "        instance=model_FD, type_cons=Cons.X_LEQ_B, variable_x='C', location_set=model_FD.locations,\n",
    "        component_set=model_FD.resources_varying_availability, b_max=scen_FD.cons_max,\n",
    "        loc_comp_dict=scen_FD.location_resource_dict, b_factor=scen_FD.availability_factor,\n",
    "        x_scale_level=scen_FD.scheduling_scale_level, b_scale_level=scen_FD.availability_scale_level,\n",
    "        label='restricts resource consumption to varying availablity')\n",
    "\n",
    "    constraint_export(instance=model_FD, scheduling_scale_level=scen_FD.scheduling_scale_level,\n",
    "                      network_scale_level=scen_FD.network_scale_level,\n",
    "                      location_transport_resource_dict=scen_FD.location_transport_resource_dict,\n",
    "                      transport_capacity_factor=scen_FD.transport_capacity_factor,\n",
    "                      transport_capacity_scale_level=scen_FD.transport_capacity_scale_level)\n",
    "\n",
    "    results_FD = solve(scenario=scen_FD, instance=model_FD, solver='gurobi', name=scen_name,\n",
    "                       solver_options=solver_options)\n",
    "\n",
    "    print(f'######################## Finished solving {scen_name} ({counter} of {len(load_scenario_dict)}) ########################')\n",
    "\n",
    "    model_vars = model_FD.component_map(ctype=Var)\n",
    "    vars_dict = {i: model_vars[i].extract_values() for i in model_vars.keys()}\n",
    "    obj_dict = {'objective': pyoval(model_FD.objective_cost_w_demand_penalty)}\n",
    "\n",
    "    FD_output_dict[scen_name] = {**vars_dict, **obj_dict}\n",
    "    \n",
    "    with open(fr\"{FD_pkl_folder}\\{len(selected_scenario_names)}\\{scen_name}.pkl\", 'wb') as file:\n",
    "        pickle.dump(FD_output_dict[scen_name], file)\n",
    "\n",
    "    exCost_FD += pyoval(model_FD.objective_cost_w_demand_penalty) * load_scenario_dict[scen_name]['prob']\n",
    "    exRes_FD += pyoval(model_FD.Demand_penalty_network[('com1_sold',0)]) * load_scenario_dict[scen_name]['prob']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2de38d03ae32a01",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "final_results_dict = {scenarios_to_select: {'Expected Cost UI': exCost_UI,\n",
    "                                            'Expected Cost FD': exCost_FD,\n",
    "                                            'Expected Resilience': 1 - exRes_FD/(20*400*90*4),\n",
    "                                            'Execution Time': start_time-end_time}}\n",
    "    \n",
    "with open(fr\"{FD_pkl_folder}\\{len(selected_scenario_names)}\\FD_{len(selected_scenario_names)}_final_results_HP.pkl\", 'wb') as file:\n",
    "    pickle.dump(final_results_dict, file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49d3364e8611a393",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "final_results_dict"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e1227bf74fc657f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "58686db2ee2d457d",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
