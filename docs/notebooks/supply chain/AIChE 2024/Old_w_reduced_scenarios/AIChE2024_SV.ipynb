{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-22T19:20:35.245059Z",
     "start_time": "2024-10-22T19:20:33.963866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0.00] Initializing mpi-sppy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyomo.environ import *\n",
    "import mpisppy.utils.sputils as sputils\n",
    "from mpisppy.opt.ef import ExtensiveForm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import sys\n",
    "sys.path.append('../../../../src')\n",
    "import pandas\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "from itertools import product\n",
    "from energiapy.components.temporal_scale import TemporalScale\n",
    "from energiapy.components.resource import Resource, VaryingResource\n",
    "from energiapy.components.process import Process, ProcessMode, VaryingProcess\n",
    "from energiapy.components.location import Location\n",
    "from energiapy.components.transport import Transport, VaryingTransport\n",
    "from energiapy.components.network import Network\n",
    "from energiapy.components.scenario import Scenario\n",
    "# from energiapy.model.constraints.demand import constraint_demand2\n",
    "from energiapy.components.result import Result\n",
    "from energiapy.model.formulate import formulate, Constraints, Objective\n",
    "from energiapy.plot import plot_results, plot_scenario, plot_location\n",
    "from energiapy.model.solve import solve\n",
    "from pyomo.environ import Param\n",
    "from energiapy.utils.scale_utils import scale_pyomo_set\n",
    "from energiapy.utils.scale_utils import scale_list, scale_tuple\n",
    "from energiapy.model.constraints.constraints import make_constraint, Cons\n",
    "from energiapy.model.formulate import constraint_export\n",
    "from functools import reduce\n",
    "import pickle\n",
    "from pyomo.environ import value as pyoval\n",
    "import numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89d432af6d575cf1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-22T19:20:35.260066Z",
     "start_time": "2024-10-22T19:20:35.246058Z"
    }
   },
   "outputs": [],
   "source": [
    "# _time_intervals = 7  # Number of time intervals in a planning horizon    (L_chi)\n",
    "_exec_scenarios = 4  # Number of execution scenarios                     (chi)\n",
    "\n",
    "M = 1e5  # Big M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ead8cfcacedac0f6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-22T19:20:36.531084Z",
     "start_time": "2024-10-22T19:20:36.521702Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_list(n: int):\n",
    "    \"\"\" n(int): number of planning periods\n",
    "    \"\"\"\n",
    "    l = [1]*n\n",
    "    for i in range(0, random.randint(0,n)):\n",
    "        l[i] = random.random() \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60d6b2af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T19:20:37.452699Z",
     "start_time": "2024-10-22T19:20:37.437699Z"
    }
   },
   "outputs": [],
   "source": [
    "def dist_dict(n: int):\n",
    "\n",
    "    # If event names are same before '_'; they are considered mutually exclusive\n",
    "    return {\n",
    "        'cap2':  pandas.DataFrame(data={('loc2', 'com1_process'): create_list(n)}),\n",
    "        \n",
    "        'cap4': pandas.DataFrame(data={('loc4', 'com1_process'): create_list(n)}),\n",
    "        \n",
    "        'cap7':   pandas.DataFrame(data={('loc7', 'com1_process'): create_list(n)}),\n",
    "        \n",
    "        'res1':  pandas.DataFrame(data={('loc1', 'com1_pur'): create_list(n)}),\n",
    "        \n",
    "        'res6':  pandas.DataFrame(data={('loc6', 'com1_pur'): create_list(n)}),\n",
    "        \n",
    "        'trans12': pandas.DataFrame(data={('trans12', 'com1_loc1_out'): create_list(n)}),\n",
    "\n",
    "        'trans25':pandas.DataFrame(data={('trans25', 'com1_loc2_out'): create_list(n)}),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a02c126d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T19:20:38.892599Z",
     "start_time": "2024-10-22T19:20:38.881600Z"
    }
   },
   "outputs": [],
   "source": [
    "a= numpy.random.normal(30, 5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f098f37b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T19:20:39.736312Z",
     "start_time": "2024-10-22T19:20:39.723314Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.73846385, 0.69571877, 0.78238234, 0.88213818, 0.60737149,\n       0.45391356, 0.70220827, 0.82590694, 0.56780666, 0.78160455,\n       0.71470399, 0.74460061, 0.74025013, 0.74028136, 0.77938865,\n       0.99255313, 0.70948367, 0.73535706, 0.63921512, 0.68896424,\n       0.8084178 , 0.60301256, 0.79262614, 0.68775244, 0.81885051,\n       0.72605499, 0.67876383, 0.43539532, 0.64996004, 0.38346964,\n       0.74207953, 0.74434714, 0.72032934, 0.79957299, 0.83342506,\n       0.66661293, 0.74967653, 0.72132049, 0.44589536, 0.54502238,\n       0.6712284 , 0.7658245 , 0.64308585, 0.78987582, 0.68053069,\n       0.66055741, 0.68800924, 0.90733336, 0.81804982, 0.81456074,\n       0.51354859, 0.93131529, 0.82683213, 0.6804701 , 0.39237806,\n       0.72201788, 0.89111138, 0.93781407, 0.58826979, 0.8837421 ,\n       0.66923755, 0.52506064, 0.82195005, 0.83314   , 0.80839309,\n       0.64791373, 0.80902851, 0.73238966, 0.82721978, 0.6197362 ,\n       0.72007696, 0.8347202 , 0.88445988, 0.71352901, 0.77187345,\n       0.58989648, 0.71216017, 0.54156882, 0.62606285, 0.75952228,\n       0.61651684, 0.77420509, 0.6944307 , 0.66814617, 0.53517359,\n       0.87177959, 0.6077162 , 0.61422571, 0.64910638, 0.78962701,\n       0.69437864, 0.66804158, 0.64781018, 0.71870304, 0.87900557,\n       0.73694581, 1.        , 0.66757978, 0.57477065, 0.69758673])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a/a.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2717f7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T19:20:41.722794Z",
     "start_time": "2024-10-22T19:20:40.929560Z"
    }
   },
   "outputs": [],
   "source": [
    "from energiapy.fitting.dist import fit  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3a1b97f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T19:20:41.895919Z",
     "start_time": "2024-10-22T19:20:41.723823Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[distfit] >INFO> fit\n",
      "[distfit] >INFO> transform\n",
      "[distfit] >INFO> [norm      ] [0.00 sec] [RSS: 0.490305] [loc=0.537 scale=0.196]\n",
      "[distfit] >INFO> [expon     ] [0.00 sec] [RSS: 9.66341] [loc=0.000 scale=0.537]\n",
      "[distfit] >INFO> [pareto    ] [0.00 sec] [RSS: 9.66341] [loc=-134217728.000 scale=134217728.000]\n",
      "[distfit] >INFO> [dweibull  ] [0.00 sec] [RSS: 0.256993] [loc=0.540 scale=0.157]\n",
      "[distfit] >INFO> [t         ] [0.04 sec] [RSS: 0.348674] [loc=0.541 scale=0.176]\n",
      "[distfit] >INFO> [genextreme] [0.02 sec] [RSS: 0.6305] [loc=0.472 scale=0.205]\n",
      "[distfit] >INFO> [gamma     ] [0.01 sec] [RSS: 0.532331] [loc=-2.917 scale=0.011]\n",
      "[distfit] >INFO> [lognorm   ] [0.01 sec] [RSS: 15.0132] [loc=-0.000 scale=0.000]\n",
      "[distfit] >INFO> [beta      ] [0.01 sec] [RSS: 0.650715] [loc=-371925.102 scale=371926.571]\n",
      "[distfit] >INFO> [uniform   ] [0.0 sec] [RSS: 5.28] [loc=0.000 scale=1.000]\n",
      "[distfit] >INFO> [loggamma  ] [0.00 sec] [RSS: 0.532233] [loc=-1.136 scale=0.676]\n",
      "[distfit] >INFO> Compute confidence intervals [parametric]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best fitting distribution is {'name': 'dweibull', 'score': 0.2569932898977568, 'loc': 0.5400586955897428, 'scale': 0.15674629664530834, 'arg': (1.1322487234282055,), 'params': (1.1322487234282055, 0.5400586955897428, 0.15674629664530834), 'model': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x000002C7A0651BD0>, 'bootstrap_score': 0, 'bootstrap_pass': None, 'color': '#e41a1c', 'CII_min_alpha': 0.21263845594324587, 'CII_max_alpha': 0.8674789352362398}\n"
     ]
    },
    {
     "data": {
      "text/plain": "                score            loc          scale  \\\nname                                                  \ndweibull     0.256993       0.540059       0.156746   \nt            0.348674       0.541388       0.175774   \nnorm         0.490305       0.536684       0.196123   \nloggamma     0.532233      -1.135816       0.675674   \ngamma        0.532331      -2.916946       0.011475   \ngenextreme     0.6305       0.471969       0.204888   \nbeta         0.650715 -371925.101752  371926.570772   \nuniform          5.28            0.0            1.0   \npareto       9.663407   -134217728.0    134217728.0   \nexpon        9.663408            0.0       0.536684   \nlognorm     15.013243           -0.0       0.000289   \n\n                                                arg  \\\nname                                                  \ndweibull                      (1.1322487234282055,)   \nt                               (9.75126351718269,)   \nnorm                                             ()   \nloggamma                      (12.381851754773034,)   \ngamma                           (300.999097658696,)   \ngenextreme                    (0.3331615577739272,)   \nbeta        (8615283.504665416, 21.605241890172152)   \nuniform                                          ()   \npareto                         (250087224.0112595,)   \nexpon                                            ()   \nlognorm                        (74.00200170823578,)   \n\n                                                       params  \\\nname                                                            \ndweibull    (1.1322487234282055, 0.5400586955897428, 0.156...   \nt           (9.75126351718269, 0.5413877841638541, 0.17577...   \nnorm                 (0.5366836667637301, 0.1961233295006309)   \nloggamma    (12.381851754773034, -1.1358159466940014, 0.67...   \ngamma       (300.999097658696, -2.9169458064519933, 0.0114...   \ngenextreme  (0.3331615577739272, 0.47196853874976197, 0.20...   \nbeta        (8615283.504665416, 21.605241890172152, -37192...   \nuniform                                            (0.0, 1.0)   \npareto      (250087224.0112595, -134217728.0, 134217727.99...   \nexpon                               (0.0, 0.5366836667637301)   \nlognorm     (74.00200170823578, -5e-324, 0.000288714339280...   \n\n                                                        model bootstrap_score  \\\nname                                                                            \ndweibull    <scipy.stats._distn_infrastructure.rv_continuo...               0   \nt           <scipy.stats._distn_infrastructure.rv_continuo...               0   \nnorm        <scipy.stats._distn_infrastructure.rv_continuo...               0   \nloggamma    <scipy.stats._distn_infrastructure.rv_continuo...               0   \ngamma       <scipy.stats._distn_infrastructure.rv_continuo...               0   \ngenextreme  <scipy.stats._distn_infrastructure.rv_continuo...               0   \nbeta        <scipy.stats._distn_infrastructure.rv_continuo...               0   \nuniform     <scipy.stats._distn_infrastructure.rv_continuo...               0   \npareto      <scipy.stats._distn_infrastructure.rv_continuo...               0   \nexpon       <scipy.stats._distn_infrastructure.rv_continuo...               0   \nlognorm     <scipy.stats._distn_infrastructure.rv_continuo...               0   \n\n           bootstrap_pass    color  \nname                                \ndweibull             None  #e41a1c  \nt                    None  #e41a1c  \nnorm                 None  #377eb8  \nloggamma             None  #4daf4a  \ngamma                None  #984ea3  \ngenextreme           None  #ff7f00  \nbeta                 None  #ffff33  \nuniform              None  #a65628  \npareto               None  #f781bf  \nexpon                None  #999999  \nlognorm              None  #999999  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>score</th>\n      <th>loc</th>\n      <th>scale</th>\n      <th>arg</th>\n      <th>params</th>\n      <th>model</th>\n      <th>bootstrap_score</th>\n      <th>bootstrap_pass</th>\n      <th>color</th>\n    </tr>\n    <tr>\n      <th>name</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>dweibull</th>\n      <td>0.256993</td>\n      <td>0.540059</td>\n      <td>0.156746</td>\n      <td>(1.1322487234282055,)</td>\n      <td>(1.1322487234282055, 0.5400586955897428, 0.156...</td>\n      <td>&lt;scipy.stats._distn_infrastructure.rv_continuo...</td>\n      <td>0</td>\n      <td>None</td>\n      <td>#e41a1c</td>\n    </tr>\n    <tr>\n      <th>t</th>\n      <td>0.348674</td>\n      <td>0.541388</td>\n      <td>0.175774</td>\n      <td>(9.75126351718269,)</td>\n      <td>(9.75126351718269, 0.5413877841638541, 0.17577...</td>\n      <td>&lt;scipy.stats._distn_infrastructure.rv_continuo...</td>\n      <td>0</td>\n      <td>None</td>\n      <td>#e41a1c</td>\n    </tr>\n    <tr>\n      <th>norm</th>\n      <td>0.490305</td>\n      <td>0.536684</td>\n      <td>0.196123</td>\n      <td>()</td>\n      <td>(0.5366836667637301, 0.1961233295006309)</td>\n      <td>&lt;scipy.stats._distn_infrastructure.rv_continuo...</td>\n      <td>0</td>\n      <td>None</td>\n      <td>#377eb8</td>\n    </tr>\n    <tr>\n      <th>loggamma</th>\n      <td>0.532233</td>\n      <td>-1.135816</td>\n      <td>0.675674</td>\n      <td>(12.381851754773034,)</td>\n      <td>(12.381851754773034, -1.1358159466940014, 0.67...</td>\n      <td>&lt;scipy.stats._distn_infrastructure.rv_continuo...</td>\n      <td>0</td>\n      <td>None</td>\n      <td>#4daf4a</td>\n    </tr>\n    <tr>\n      <th>gamma</th>\n      <td>0.532331</td>\n      <td>-2.916946</td>\n      <td>0.011475</td>\n      <td>(300.999097658696,)</td>\n      <td>(300.999097658696, -2.9169458064519933, 0.0114...</td>\n      <td>&lt;scipy.stats._distn_infrastructure.rv_continuo...</td>\n      <td>0</td>\n      <td>None</td>\n      <td>#984ea3</td>\n    </tr>\n    <tr>\n      <th>genextreme</th>\n      <td>0.6305</td>\n      <td>0.471969</td>\n      <td>0.204888</td>\n      <td>(0.3331615577739272,)</td>\n      <td>(0.3331615577739272, 0.47196853874976197, 0.20...</td>\n      <td>&lt;scipy.stats._distn_infrastructure.rv_continuo...</td>\n      <td>0</td>\n      <td>None</td>\n      <td>#ff7f00</td>\n    </tr>\n    <tr>\n      <th>beta</th>\n      <td>0.650715</td>\n      <td>-371925.101752</td>\n      <td>371926.570772</td>\n      <td>(8615283.504665416, 21.605241890172152)</td>\n      <td>(8615283.504665416, 21.605241890172152, -37192...</td>\n      <td>&lt;scipy.stats._distn_infrastructure.rv_continuo...</td>\n      <td>0</td>\n      <td>None</td>\n      <td>#ffff33</td>\n    </tr>\n    <tr>\n      <th>uniform</th>\n      <td>5.28</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>()</td>\n      <td>(0.0, 1.0)</td>\n      <td>&lt;scipy.stats._distn_infrastructure.rv_continuo...</td>\n      <td>0</td>\n      <td>None</td>\n      <td>#a65628</td>\n    </tr>\n    <tr>\n      <th>pareto</th>\n      <td>9.663407</td>\n      <td>-134217728.0</td>\n      <td>134217728.0</td>\n      <td>(250087224.0112595,)</td>\n      <td>(250087224.0112595, -134217728.0, 134217727.99...</td>\n      <td>&lt;scipy.stats._distn_infrastructure.rv_continuo...</td>\n      <td>0</td>\n      <td>None</td>\n      <td>#f781bf</td>\n    </tr>\n    <tr>\n      <th>expon</th>\n      <td>9.663408</td>\n      <td>0.0</td>\n      <td>0.536684</td>\n      <td>()</td>\n      <td>(0.0, 0.5366836667637301)</td>\n      <td>&lt;scipy.stats._distn_infrastructure.rv_continuo...</td>\n      <td>0</td>\n      <td>None</td>\n      <td>#999999</td>\n    </tr>\n    <tr>\n      <th>lognorm</th>\n      <td>15.013243</td>\n      <td>-0.0</td>\n      <td>0.000289</td>\n      <td>(74.00200170823578,)</td>\n      <td>(74.00200170823578, -5e-324, 0.000288714339280...</td>\n      <td>&lt;scipy.stats._distn_infrastructure.rv_continuo...</td>\n      <td>0</td>\n      <td>None</td>\n      <td>#999999</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2518a61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T19:20:45.263588Z",
     "start_time": "2024-10-22T19:20:45.248764Z"
    }
   },
   "outputs": [],
   "source": [
    "ll = 4\n",
    "d = {i: dist_dict(4) for i in range(ll)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "166d5c94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T19:20:45.544703Z",
     "start_time": "2024-10-22T19:20:45.515859Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{0: {'cap2':           loc2\n    com1_process\n  0     0.093138\n  1     0.046301\n  2     1.000000\n  3     1.000000,\n  'cap4':           loc4\n    com1_process\n  0     0.898920\n  1     0.203022\n  2     1.000000\n  3     1.000000,\n  'cap7':           loc7\n    com1_process\n  0            1\n  1            1\n  2            1\n  3            1,\n  'res1':        loc1\n     com1_pur\n  0  0.483077\n  1  1.000000\n  2  1.000000\n  3  1.000000,\n  'res6':        loc6\n     com1_pur\n  0  0.871313\n  1  0.975788\n  2  0.924420\n  3  1.000000,\n  'trans12':         trans12\n    com1_loc1_out\n  0      0.332611\n  1      0.436909\n  2      0.493426\n  3      0.157839,\n  'trans25':         trans25\n    com1_loc2_out\n  0             1\n  1             1\n  2             1\n  3             1},\n 1: {'cap2':           loc2\n    com1_process\n  0            1\n  1            1\n  2            1\n  3            1,\n  'cap4':           loc4\n    com1_process\n  0     0.336859\n  1     1.000000\n  2     1.000000\n  3     1.000000,\n  'cap7':           loc7\n    com1_process\n  0     0.196603\n  1     1.000000\n  2     1.000000\n  3     1.000000,\n  'res1':       loc1\n    com1_pur\n  0        1\n  1        1\n  2        1\n  3        1,\n  'res6':        loc6\n     com1_pur\n  0  0.058800\n  1  0.362094\n  2  0.585887\n  3  1.000000,\n  'trans12':         trans12\n    com1_loc1_out\n  0      0.854557\n  1      0.721316\n  2      1.000000\n  3      1.000000,\n  'trans25':         trans25\n    com1_loc2_out\n  0      0.415410\n  1      0.245297\n  2      0.505804\n  3      0.400869},\n 2: {'cap2':           loc2\n    com1_process\n  0            1\n  1            1\n  2            1\n  3            1,\n  'cap4':           loc4\n    com1_process\n  0     0.422987\n  1     1.000000\n  2     1.000000\n  3     1.000000,\n  'cap7':           loc7\n    com1_process\n  0     0.519435\n  1     0.892149\n  2     0.355243\n  3     1.000000,\n  'res1':        loc1\n     com1_pur\n  0  0.718904\n  1  1.000000\n  2  1.000000\n  3  1.000000,\n  'res6':        loc6\n     com1_pur\n  0  0.186147\n  1  0.170932\n  2  0.783163\n  3  0.426409,\n  'trans12':         trans12\n    com1_loc1_out\n  0      0.553761\n  1      1.000000\n  2      1.000000\n  3      1.000000,\n  'trans25':         trans25\n    com1_loc2_out\n  0             1\n  1             1\n  2             1\n  3             1},\n 3: {'cap2':           loc2\n    com1_process\n  0     0.139824\n  1     0.764015\n  2     0.857170\n  3     0.814171,\n  'cap4':           loc4\n    com1_process\n  0     0.648656\n  1     0.223453\n  2     0.251469\n  3     1.000000,\n  'cap7':           loc7\n    com1_process\n  0            1\n  1            1\n  2            1\n  3            1,\n  'res1':        loc1\n     com1_pur\n  0  0.994145\n  1  0.317539\n  2  1.000000\n  3  1.000000,\n  'res6':        loc6\n     com1_pur\n  0  0.553334\n  1  1.000000\n  2  1.000000\n  3  1.000000,\n  'trans12':         trans12\n    com1_loc1_out\n  0      0.302509\n  1      0.667679\n  2      0.599709\n  3      0.081828,\n  'trans25':         trans25\n    com1_loc2_out\n  0      0.304179\n  1      1.000000\n  2      1.000000\n  3      1.000000}}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ac3532d38ed748",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_event_dict(n: int):\n",
    "    default_list = [1] * n\n",
    "    rands = numpy.random.rand(n)\n",
    "\n",
    "    rands = rands/rands.sum()\n",
    "\n",
    "    # If event names are same before '_'; they are considered mutually exclusive\n",
    "    event_dict = {\n",
    "        'cap2_1': {'prob': rands[0], 'factor': pandas.DataFrame(data={('loc2', 'com1_process'): create_list(n)})},\n",
    "        'cap2_2': {'prob': rands[1], 'factor': pandas.DataFrame(data={('loc2', 'com1_process'): create_list(n)})},\n",
    "        'cap2_3': {'prob': rands[2],'factor': pandas.DataFrame(data={('loc2', 'com1_process'): create_list(n)})},\n",
    "\n",
    "        'cap4_1': {'prob': rands[0], 'factor': pandas.DataFrame(data={('loc4', 'com1_process'): create_list(n)})},\n",
    "        'cap4_2': {'prob': rands[1], 'factor': pandas.DataFrame(data={('loc4', 'com1_process'): create_list(n)})},\n",
    "        'cap4_3': {'prob': rands[2], 'factor': pandas.DataFrame(data={('loc4', 'com1_process'): create_list(n)})},\n",
    "\n",
    "        'cap7_1': {'prob': rands[0],  'factor': pandas.DataFrame(data={('loc7', 'com1_process'): create_list(n)})},\n",
    "        'cap7_2': {'prob': rands[1],  'factor': pandas.DataFrame(data={('loc7', 'com1_process'): create_list(n)})},\n",
    "        'cap7_3': {'prob': rands[2], 'factor': pandas.DataFrame(data={('loc7', 'com1_process'): create_list(n)})},\n",
    "\n",
    "        'res1_1': {'prob': rands[0], 'factor': pandas.DataFrame(data={('loc1', 'com1_pur'): create_list(n)})},\n",
    "        'res1_2': {'prob': rands[1], 'factor': pandas.DataFrame(data={('loc1', 'com1_pur'): create_list(n)})},\n",
    "        'res1_3': {'prob': rands[2], 'factor': pandas.DataFrame(data={('loc1', 'com1_pur'): create_list(n)})},\n",
    "\n",
    "        'res6_1': {'prob': rands[0], 'factor': pandas.DataFrame(data={('loc6', 'com1_pur'): create_list(n)})},\n",
    "        'res6_2': {'prob': rands[1], 'factor': pandas.DataFrame(data={('loc6', 'com1_pur'): create_list(n)})},\n",
    "        'res6_3': {'prob': rands[2], 'factor': pandas.DataFrame(data={('loc6', 'com1_pur'): create_list(n)})},\n",
    "\n",
    "\n",
    "        'trans12_2': {'prob':  random.random(), 'factor': pandas.DataFrame(data={('trans12', 'com1_loc1_out'): create_list(n)})},\n",
    "\n",
    "        'trans25_2': {'prob':  random.random(), 'factor': pandas.DataFrame(data={('trans25', 'com1_loc2_out'): create_list(n)})},\n",
    "    }\n",
    "\n",
    "    nd_dict = {\n",
    "        'trans25_nd': {'prob': 1 - event_dict['trans25_2']['prob'], 'factor': pandas.DataFrame(data={('trans25', 'com1_loc2_out'): default_list})},\n",
    "        'trans12_nd': {'prob': 1 - event_dict['trans12_2']['prob'], 'factor': pandas.DataFrame(data={('trans12', 'com1_loc1_out'): default_list})},\n",
    "        'res6_nd': {'prob':   rands[3], 'factor': pandas.DataFrame(data={('loc6', 'com1_pur'): default_list})},\n",
    "        'res1_nd': {'prob':   rands[3], 'factor': pandas.DataFrame(data={('loc1', 'com1_pur'): default_list})},\n",
    "        'cap7_nd': {'prob':   rands[3], 'factor': pandas.DataFrame(data={('loc7', 'com1_process'): default_list})},\n",
    "        'cap2_nd': {'prob':   rands[3], 'factor': pandas.DataFrame(data={('loc2', 'com1_process'): default_list})},\n",
    "        'cap4_nd': {'prob':   rands[3], 'factor': pandas.DataFrame(data={('loc4', 'com1_process'): default_list})},\n",
    "    }\n",
    "    return {**event_dict, **nd_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef031e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c908361cf3196b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def remove_scenarios(input_tuples, tuples_to_check):\n",
    "#     filtered_tuples = []\n",
    "# \n",
    "#     for s_tuple in input_tuples:\n",
    "#         should_remove = False\n",
    "# \n",
    "#         # Join the tuple elements into a single string for easier processing\n",
    "#         s = ' '.join(s_tuple)\n",
    "# \n",
    "#         # Loop through each tuple in the list of tuples to check\n",
    "#         for tuple_group in tuples_to_check:\n",
    "#             all_have_suffix_1 = True  # Assume all have '_1'\n",
    "# \n",
    "#             # Loop through each element in the tuple\n",
    "#             for res in tuple_group:\n",
    "#                 # Build regex pattern to match occurrences like 'res1_1' exactly\n",
    "#                 res_pattern = fr'{res}_1'\n",
    "# \n",
    "#                 # Check if the pattern 'res_x_1' is in the joined string\n",
    "#                 if not re.search(res_pattern, s):\n",
    "#                     all_have_suffix_1 = False\n",
    "#                     break\n",
    "# \n",
    "#             # If all elements in the tuple have '_1', mark for removal\n",
    "#             if all_have_suffix_1:\n",
    "#                 should_remove = True\n",
    "#                 break\n",
    "# \n",
    "#         # If the tuple doesn't meet removal criteria, add it to the filtered list\n",
    "#         if not should_remove:\n",
    "#             filtered_tuples.append(s_tuple)\n",
    "# \n",
    "#     return filtered_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c54fae4ab2dc8a8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_scenarios(input_tuples, tuples_to_check):\n",
    "    filtered_tuples = []\n",
    "\n",
    "    for s_tuple in input_tuples:\n",
    "        should_remove = False\n",
    "\n",
    "        # Join the tuple elements into a single string for easier processing\n",
    "        s = ' '.join(s_tuple)\n",
    "\n",
    "        # Loop through each tuple in the list of tuples to check\n",
    "        for tuple_group in tuples_to_check:\n",
    "            all_present = True\n",
    "            any_nom = False\n",
    "\n",
    "            # Loop through each element in the tuple\n",
    "            for res in tuple_group:\n",
    "                # Build regex pattern to match occurrences like 'res1_x' where x can be any number\n",
    "                res_pattern = fr'{res}(_\\d+)?'\n",
    "\n",
    "                # Check if the pattern is in the joined string\n",
    "                if not re.search(res_pattern, s):\n",
    "                    all_present = False\n",
    "                    break\n",
    "\n",
    "                # Check if '_nom' is in the string\n",
    "                if f'{res}_nd' in s:\n",
    "                    any_nom = True\n",
    "\n",
    "            # If all elements are present and none of them have '_nom', mark for removal\n",
    "            if all_present and not any_nom:\n",
    "                should_remove = True\n",
    "                break\n",
    "\n",
    "        # If the tuple doesn't meet removal criteria, add it to the filtered list\n",
    "        if not should_remove:\n",
    "            filtered_tuples.append(s_tuple)\n",
    "\n",
    "    return filtered_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3fe6f516d80bab",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to generate the scenario dictionary for n sets of events\n",
    "def create_scenario_dict(event_dict, remove=None):\n",
    "    # Extract unique event prefixes (e.g., 'cap2', 'cap4', ...)\n",
    "    event_prefixes = set(key.split('_')[0] for key in event_dict)\n",
    "\n",
    "    # Group events by their prefixes\n",
    "    grouped_events = {prefix: [key for key in event_dict if key.startswith(prefix)] for prefix in event_prefixes}\n",
    "\n",
    "    # Create all possible combinations of events across the different groups\n",
    "    filtered_event_combinations = list(product(*grouped_events.values()))\n",
    "    # print(filtered_event_combinations)\n",
    "\n",
    "    if remove is not None:\n",
    "        filtered_event_combinations = remove_scenarios(filtered_event_combinations, remove)\n",
    "    # print(filtered_event_combinations)\n",
    "\n",
    "    scenario_dict = {}\n",
    "\n",
    "    # Iterate over all event combinations\n",
    "    for combination in filtered_event_combinations:\n",
    "        # Construct the scenario key\n",
    "        scenario_key = ' '.join(combination)\n",
    "\n",
    "        # Calculate the probability of this scenario\n",
    "        prob = 1\n",
    "        combined_factor = None\n",
    "\n",
    "        for event_key in combination:\n",
    "            # Multiply probabilities\n",
    "            prob *= event_dict[event_key]['prob']\n",
    "\n",
    "            # Combine factors (assumes they are pandas DataFrames)\n",
    "            if combined_factor is None:\n",
    "                combined_factor = event_dict[event_key]['factor'].copy()\n",
    "            else:\n",
    "                combined_factor = combined_factor.add(event_dict[event_key]['factor'], fill_value=0)\n",
    "\n",
    "        # Add to the scenario dictionary\n",
    "        scenario_dict[scenario_key] = {'prob': prob, 'factor': combined_factor}\n",
    "        norm_factor = sum(data['prob'] for key, data in scenario_dict.items())\n",
    "\n",
    "        for key in scenario_dict:\n",
    "            scenario_dict[key]['prob'] = scenario_dict[key]['prob']/norm_factor\n",
    "\n",
    "    return scenario_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c37136f862fcfe",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(scen_df=pandas.DataFrame()):\n",
    "    default_df = pandas.DataFrame(data=[1] * _exec_scenarios)\n",
    "    scale_factor = 90\n",
    "\n",
    "    # Define temporal scales\n",
    "    scales = TemporalScale(discretization_list=[1, _exec_scenarios])\n",
    "\n",
    "    # ======================================================================================================================\n",
    "    # Declare resources/commodities\n",
    "    # ======================================================================================================================\n",
    "    com1_pur = Resource(name='com1_pur', cons_max=225*scale_factor, block={'imp': 1, 'urg': 1}, price=0.00,\n",
    "                        label='Commodity 1 consumed from outside the system',\n",
    "                        varying=[VaryingResource.DETERMINISTIC_AVAILABILITY])\n",
    "\n",
    "    com1_in = Resource(name='com1_in', label='Commodity 1 received')\n",
    "    com1_out = Resource(name='com1_out', label='Commodity 1 to be sent out')\n",
    "\n",
    "    com1_loc1_out = Resource(name='com1_loc1_out', label='Commodity 1 sent out from location 1')\n",
    "    com1_loc2_out = Resource(name='com1_loc2_out', label='Commodity 1 sent out from location 2')\n",
    "    com1_loc3_out = Resource(name='com1_loc3_out', label='Commodity 1 sent out from location 3')\n",
    "    com1_loc4_out = Resource(name='com1_loc4_out', label='Commodity 1 sent out from location 4')\n",
    "    com1_loc5_out = Resource(name='com1_loc5_out', label='Commodity 1 sent out from location 5')\n",
    "    com1_loc6_out = Resource(name='com1_loc6_out', label='Commodity 1 sent out from location 6')\n",
    "    com1_loc7_out = Resource(name='com1_loc7_out', label='Commodity 1 sent out from location 7')\n",
    "\n",
    "    com1_sold = Resource(name='com1_sold', revenue=0.00, demand=True, sell=True,\n",
    "                         label='Commodity 1 sold to outside the system')\n",
    "\n",
    "    # ======================================================================================================================\n",
    "    # Declare processes/storage capacities\n",
    "    # ======================================================================================================================\n",
    "    com1_process_capacity = 500*scale_factor\n",
    "\n",
    "    # prod_max = {0: 0.25*com1_process_capacity, 1: 0.5*com1_process_capacity, 2: 0.75*com1_process_capacity, 3: 0.95*com1_process_capacity, 4: com1_process_capacity}\n",
    "    # prod_min = {0: 0, 1: 0.25*com1_process_capacity, 2: 0.5*com1_process_capacity, 3: 0.75*com1_process_capacity, 4: 0.95*com1_process_capacity}\n",
    "    # rate_max = {0:1.25/2, 1: 1/2, 2: 0.75/2, 3: 0.5/2, 4: 0.25/2}\n",
    "    # mode_ramp = {(0,1): 5, (1,2): 5}\n",
    "\n",
    "    com1_procure = Process(name='procure com1', prod_max=com1_process_capacity, conversion={com1_pur: -1, com1_in: 1},\n",
    "                           capex=25/scale_factor, vopex=0.01, prod_min=0.01, label='Procure com1')\n",
    "    com1_sell = Process(name='sell com1', prod_max=com1_process_capacity, conversion={com1_out: -1, com1_sold: 1},\n",
    "                        capex=0.1/scale_factor, vopex=0.01, prod_min=0.01, label='Sell com1')\n",
    "    # com1_opt_procure = Process(name='procure optional com1', prod_max=75, conversion={com1_pur: -1, com1_in:1}, capex=10, vopex=0.1, prod_min=0.01, label='Procure optional com1')\n",
    "\n",
    "    com1_receive_loc1 = Process(name='com1_receive_loc1', prod_max=com1_process_capacity,\n",
    "                                conversion={com1_loc1_out: -1, com1_in: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                                label='Commodity 1 received from location 1')\n",
    "    com1_receive_loc2 = Process(name='com1_receive_loc2', prod_max=com1_process_capacity,\n",
    "                                conversion={com1_loc2_out: -1, com1_in: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                                label='Commodity 1 received from location 2')\n",
    "    com1_receive_loc3 = Process(name='com1_receive_loc3', prod_max=com1_process_capacity,\n",
    "                                conversion={com1_loc3_out: -1, com1_in: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                                label='Commodity 1 received from location 3')\n",
    "    com1_receive_loc4 = Process(name='com1_receive_loc4', prod_max=com1_process_capacity,\n",
    "                                conversion={com1_loc4_out: -1, com1_in: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                                label='Commodity 1 received from location 4')\n",
    "    com1_receive_loc5 = Process(name='com1_receive_loc5', prod_max=com1_process_capacity,\n",
    "                                conversion={com1_loc5_out: -1, com1_in: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                                label='Commodity 1 received from location 5')\n",
    "    com1_receive_loc6 = Process(name='com1_receive_loc6', prod_max=com1_process_capacity,\n",
    "                                conversion={com1_loc6_out: -1, com1_in: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                                label='Commodity 1 received from location 6')\n",
    "    com1_receive_loc7 = Process(name='com1_receive_loc7', prod_max=com1_process_capacity,\n",
    "                                conversion={com1_loc7_out: -1, com1_in: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                                label='Commodity 1 received from location 7')\n",
    "\n",
    "    com1_process = Process(name='com1_process', prod_max=com1_process_capacity, conversion={com1_in: -1, com1_out: 1},\n",
    "                           capex=5/scale_factor, vopex=0.01, prod_min=0.01, label='Process the commodity through the location',\n",
    "                           varying=[VaryingProcess.DETERMINISTIC_CAPACITY])\n",
    "\n",
    "    com1_store = Process(name='com1_store', prod_max=com1_process_capacity, capex=0.5/scale_factor, vopex=0.01, storage_capex=30/scale_factor, store_min=0.01,\n",
    "                         store_max=200*scale_factor, prod_min=0.01, label=\"Storage process\", storage=com1_in, storage_cost=0.02)\n",
    "\n",
    "    com1_loc1_send = Process(name='com1_loc1_send', prod_max=com1_process_capacity,\n",
    "                             conversion={com1_out: -1, com1_loc1_out: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                             label='Send commodity one from location 1')\n",
    "    com1_loc2_send = Process(name='com1_loc2_send', prod_max=com1_process_capacity,\n",
    "                             conversion={com1_out: -1, com1_loc2_out: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                             label='Send commodity one from location 2')\n",
    "    com1_loc3_send = Process(name='com1_loc3_send', prod_max=com1_process_capacity,\n",
    "                             conversion={com1_out: -1, com1_loc3_out: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                             label='Send commodity one from location 3')\n",
    "    com1_loc4_send = Process(name='com1_loc4_send', prod_max=com1_process_capacity,\n",
    "                             conversion={com1_out: -1, com1_loc4_out: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                             label='Send commodity one from location 4')\n",
    "    com1_loc5_send = Process(name='com1_loc5_send', prod_max=com1_process_capacity,\n",
    "                             conversion={com1_out: -1, com1_loc5_out: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                             label='Send commodity one from location 5')\n",
    "    com1_loc6_send = Process(name='com1_loc6_send', prod_max=com1_process_capacity,\n",
    "                             conversion={com1_out: -1, com1_loc6_out: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                             label='Send commodity one from location 6')\n",
    "    com1_loc7_send = Process(name='com1_loc7_send', prod_max=com1_process_capacity,\n",
    "                             conversion={com1_out: -1, com1_loc7_out: 1}, capex=0.1/scale_factor, vopex=0.01, prod_min=0.01,\n",
    "                             label='Send commodity one from location 7')\n",
    "\n",
    "    # ======================================================================================================================\n",
    "    # Declare locations/warehouses\n",
    "    # ======================================================================================================================\n",
    "    loc1 = Location(name='loc1',\n",
    "                    processes={com1_procure, com1_receive_loc2, com1_receive_loc3, com1_process, com1_store,\n",
    "                               com1_loc1_send}, label=\"Location 1\",\n",
    "                    scales=scales, demand_scale_level=1, capacity_scale_level=1, availability_scale_level=1,\n",
    "                    availability_factor={\n",
    "                        com1_pur: scen_df[[('loc1', 'com1_pur')]] if ('loc1', 'com1_pur') in scen_df else default_df})\n",
    "\n",
    "    loc2 = Location(name='loc2',\n",
    "                    processes={com1_receive_loc1, com1_receive_loc4, com1_receive_loc5, com1_process, com1_store,\n",
    "                               com1_loc2_send}, label=\"Location 2\", scales=scales, demand_scale_level=1,\n",
    "                    capacity_scale_level=1, availability_scale_level=1,\n",
    "                    capacity_factor={com1_process: scen_df[[('loc2', 'com1_process')]] if ('loc2',\n",
    "                                                                                           'com1_process') in scen_df else default_df})\n",
    "\n",
    "    loc3 = Location(name='loc3',\n",
    "                    processes={com1_receive_loc1, com1_receive_loc4, com1_process, com1_store, com1_loc3_send},\n",
    "                    label=\"Location 3\", scales=scales, demand_scale_level=1, capacity_scale_level=1,\n",
    "                    availability_scale_level=1)\n",
    "\n",
    "    loc4 = Location(name='loc4', processes={com1_receive_loc2, com1_receive_loc3, com1_receive_loc6, com1_receive_loc5,\n",
    "                                            com1_receive_loc7, com1_process, com1_store, com1_loc4_send},\n",
    "                    label=\"Location 4\", scales=scales, demand_scale_level=1, capacity_scale_level=1,\n",
    "                    availability_scale_level=1,\n",
    "                    capacity_factor={com1_process: scen_df[[('loc4', 'com1_process')]] if ('loc4',\n",
    "                                                                                           'com1_process') in scen_df else default_df})\n",
    "\n",
    "    loc5 = Location(name='loc5',\n",
    "                    processes={com1_receive_loc2, com1_receive_loc4, com1_receive_loc7, com1_process, com1_store,\n",
    "                               com1_loc5_send, com1_sell}, label=\"Location 5\", scales=scales, demand_scale_level=1,\n",
    "                    capacity_scale_level=1, availability_scale_level=1)\n",
    "\n",
    "    loc6 = Location(name='loc6', processes={com1_procure, com1_receive_loc4, com1_process, com1_store, com1_loc6_send},\n",
    "                    label=\"Location 6\", scales=scales, demand_scale_level=1, capacity_scale_level=1,\n",
    "                    availability_scale_level=1,\n",
    "                    availability_factor={\n",
    "                        com1_pur: scen_df[[('loc6', 'com1_pur')]] if ('loc6', 'com1_pur') in scen_df else default_df})\n",
    "\n",
    "    loc7 = Location(name='loc7',\n",
    "                    processes={com1_receive_loc4, com1_receive_loc5, com1_process, com1_store, com1_loc7_send},\n",
    "                    label=\"Location 7\", scales=scales, demand_scale_level=1, capacity_scale_level=1,\n",
    "                    availability_scale_level=1,\n",
    "                    capacity_factor={com1_process: scen_df[[('loc7', 'com1_process')]] if ('loc7',\n",
    "                                                                                           'com1_process') in scen_df else default_df})\n",
    "\n",
    "    # ======================================================================================================================\n",
    "    # Declare transport/trucks\n",
    "    # ======================================================================================================================\n",
    "\n",
    "    truck_cap12 = 280*scale_factor\n",
    "    truck_cap13 = 270*scale_factor\n",
    "    truck_cap24 = 450*scale_factor\n",
    "    truck_cap25 = 270*scale_factor\n",
    "    truck_cap34 = 270*scale_factor\n",
    "    truck_cap45 = 500*scale_factor\n",
    "    truck_cap47 = 360*scale_factor\n",
    "    truck_cap64 = 450*scale_factor\n",
    "    truck_cap75 = 360*scale_factor\n",
    "\n",
    "    truck12 = Transport(name='truck12', resources={com1_loc1_out}, trans_max=truck_cap12,\n",
    "                        label='Truck from location 1 to 2', capex=0.5/scale_factor, vopex=0.05, trans_min=0.01, varying=[VaryingTransport.DETERMINISTIC_CAPACITY])\n",
    "    # truck21 = Transport(name='truck21', resources={com1_loc2_out}, trans_max=truck_cap12,\n",
    "    #                     label='Truck from location 2 to 1', capex=0.0001, vopex=0.05, trans_min=0.01)\n",
    "\n",
    "    truck13 = Transport(name='truck13', resources={com1_loc1_out}, trans_max=truck_cap13,\n",
    "                        label='Truck from location 1 to 3', capex=0.3/scale_factor, vopex=0.03, trans_min=0.01, varying=[VaryingTransport.DETERMINISTIC_CAPACITY])\n",
    "    # truck31 = Transport(name='truck31', resources={com1_loc3_out}, trans_max=truck_cap13,\n",
    "    #                     label='Truck from location 3 to 1', capex=0.0001, vopex=0.03, trans_min=0.01)\n",
    "\n",
    "    truck24 = Transport(name='truck24', resources={com1_loc2_out}, trans_max=truck_cap24,\n",
    "                        label='Truck from location 2 to 4', capex=0.5/scale_factor, vopex=0.05, trans_min=0.01, varying=[VaryingTransport.DETERMINISTIC_CAPACITY])\n",
    "    # truck42 = Transport(name='truck42', resources={com1_loc4_out}, trans_max=truck_cap24,\n",
    "    #                     label='Truck from location 4 to 2', capex=0.0001, vopex=0.05, trans_min=0.01)\n",
    "\n",
    "    truck25 = Transport(name='truck25', resources={com1_loc2_out}, trans_max=truck_cap25,\n",
    "                        label='Truck from location 2 to 5', capex=0.3/scale_factor, vopex=0.03, trans_min=0.01, varying=[VaryingTransport.DETERMINISTIC_CAPACITY])\n",
    "    # truck52 = Transport(name='truck52', resources={com1_loc5_out}, trans_max=truck_cap25,\n",
    "    #                     label='Truck from location 5 to 2', capex=0.0001, vopex=0.03, trans_min=0.01)\n",
    "\n",
    "    truck34 = Transport(name='truck34', resources={com1_loc3_out}, trans_max=truck_cap34,\n",
    "                        label='Truck from location 3 to 4', capex=0.2/scale_factor, vopex=0.02, trans_min=0.01, varying=[VaryingTransport.DETERMINISTIC_CAPACITY])\n",
    "    # truck43 = Transport(name='truck43', resources={com1_loc4_out}, trans_max=truck_cap34,\n",
    "    #                     label='Truck from location 4 to 3', capex=0.0001, vopex=0.02, trans_min=0.01)\n",
    "\n",
    "    truck45 = Transport(name='truck45', resources={com1_loc4_out}, trans_max=truck_cap45,\n",
    "                        label='Truck from location 4 to 5', capex=1/scale_factor, vopex=0.1, trans_min=0.01, varying=[VaryingTransport.DETERMINISTIC_CAPACITY])\n",
    "    # truck54 = Transport(name='truck54', resources={com1_loc5_out}, trans_max=truck_cap45,\n",
    "    #                     label='Truck from location 5 to 4', capex=0.0001, vopex=0.1, trans_min=0.01)\n",
    "\n",
    "    truck47 = Transport(name='truck47', resources={com1_loc4_out}, trans_max=truck_cap47,\n",
    "                        label='Truck from location 4 to 7', capex=0.4/scale_factor, vopex=0.04, trans_min=0.01, varying=[VaryingTransport.DETERMINISTIC_CAPACITY])\n",
    "    # truck74 = Transport(name='truck74', resources={com1_loc7_out}, trans_max=truck_cap47,\n",
    "    #                     label='Truck from location 7 to 4', capex=0.0001, vopex=0.04, trans_min=0.01)\n",
    "\n",
    "    truck64 = Transport(name='truck64', resources={com1_loc6_out}, trans_max=truck_cap64,\n",
    "                        label='Truck from location 6 to 4', capex=0.5/scale_factor, vopex=0.05, trans_min=0.01, varying=[VaryingTransport.DETERMINISTIC_CAPACITY])\n",
    "    # truck46 = Transport(name='truck46', resources={com1_loc4_out}, trans_max=truck_cap64,\n",
    "    #                     label='Truck from location 4 to 6', capex=0.0001, vopex=0.05, trans_min=0.01)\n",
    "\n",
    "    truck75 = Transport(name='truck75', resources={com1_loc7_out}, trans_max=truck_cap75,\n",
    "                        label='Truck from location 7 to 5', capex=0.4/scale_factor, vopex=0.04, trans_min=0.01, varying=[VaryingTransport.DETERMINISTIC_CAPACITY])\n",
    "    # truck57 = Transport(name='truck57', resources={com1_loc5_out}, trans_max=truck_cap75,\n",
    "    #                     label='Truck from location 5 to 7', capex=0.0001, vopex=0.04, trans_min=0.01)\n",
    "\n",
    "    # ======================================================================================================================\n",
    "    # Declare network\n",
    "    # ======================================================================================================================\n",
    "\n",
    "    transport_matrix = [\n",
    "        [[], [truck12], [truck13], [], [], [], []],  # source: location 1\n",
    "        [[], [], [], [truck24], [truck25], [], []],  # source: location 2\n",
    "        [[], [], [], [truck34], [], [], []],  # source: location 3\n",
    "        [[], [], [], [], [truck45], [], [truck47]],  # source: location 4\n",
    "        [[], [], [], [], [], [], []],  # source: location 5\n",
    "        [[], [], [], [truck64], [], [], []],  # source: location 6\n",
    "        [[], [], [], [], [truck75], [], []]  # source: location 7\n",
    "    ]\n",
    "\n",
    "    # transport_matrix = [\n",
    "    #     [[], [truck12], [truck13], [], [], [], []],  # source: location 1\n",
    "    #     [[truck21], [], [], [truck24], [truck25], [], []],  # source: location 2\n",
    "    #     [[truck31], [], [], [truck34], [], [], []],  # source: location 3\n",
    "    #     [[], [truck42], [truck43], [], [truck45], [truck46], [truck47]],  # source: location 4\n",
    "    #     [[], [truck52], [], [truck54], [], [], [truck57]],  # source: location 5\n",
    "    #     [[], [], [], [truck64], [], [], []],  # source: location 6\n",
    "    #     [[], [], [], [truck74], [truck75], [], []]  # source: location 7\n",
    "    # ]\n",
    "\n",
    "    distance_matrix = [\n",
    "        [0, 55, 196, M, M, M, M],\n",
    "        [55, 0, M, 163, 112, M, 134],\n",
    "        [196, M, 0, 63, M, M, M],\n",
    "        [M, 163, 63, 0, 95, 117, 88],\n",
    "        [M, 112, M, 95, 0, M, 134],\n",
    "        [M, M, M, 117, M, 0, M],\n",
    "        [M, 134, M, 88, 134, M, 0]\n",
    "    ]\n",
    "\n",
    "    locset = [loc1, loc2, loc3, loc4, loc5, loc6, loc7]\n",
    "\n",
    "    sources = locset\n",
    "    sinks = locset\n",
    "\n",
    "    network = Network(name='Network', scales=scales, source_locations=sources, sink_locations=sinks,\n",
    "                      transport_matrix=transport_matrix, distance_matrix=distance_matrix, transport_capacity_scale_level=1,\n",
    "                      transport_capacity_factor={(loc1, loc2): {truck12: scen_df[[('trans12', 'com1_loc1_out')]] if ('trans12', 'com1_loc1_out') in scen_df else default_df},\n",
    "                                                 (loc1, loc3): {truck13: scen_df[[('trans13', 'com1_loc1_out')]] if ('trans13', 'com1_loc1_out') in scen_df else default_df},\n",
    "                                                 (loc2, loc4): {truck24: scen_df[[('trans24', 'com1_loc2_out')]] if ('trans24', 'com1_loc2_out') in scen_df else default_df},\n",
    "                                                 (loc2, loc5): {truck25: scen_df[[('trans25', 'com1_loc2_out')]] if ('trans25', 'com1_loc2_out') in scen_df else default_df},\n",
    "                                                 (loc3, loc4): {truck34: scen_df[[('trans34', 'com1_loc3_out')]] if ('trans34', 'com1_loc3_out') in scen_df else default_df},\n",
    "                                                 (loc4, loc5): {truck45: scen_df[[('trans45', 'com1_loc4_out')]] if ('trans45', 'com1_loc4_out') in scen_df else default_df},\n",
    "                                                 (loc4, loc7): {truck47: scen_df[[('trans47', 'com1_loc4_out')]] if ('trans47', 'com1_loc4_out') in scen_df else default_df},\n",
    "                                                 (loc6, loc4): {truck64: scen_df[[('trans64', 'com1_loc6_out')]] if ('trans64', 'com1_loc6_out') in scen_df else default_df},\n",
    "                                                 (loc7, loc5): {truck75: scen_df[[('trans75', 'com1_loc7_out')]] if ('trans75', 'com1_loc7_out') in scen_df else default_df},\n",
    "                                                 })\n",
    "\n",
    "    # ======================================================================================================================\n",
    "    # Declare scenario\n",
    "    # ======================================================================================================================\n",
    "\n",
    "    daily_demand = 400*scale_factor\n",
    "    demand_penalty = 20\n",
    "\n",
    "    demand_dict = {i: {com1_sold: daily_demand} if i == loc5 else {com1_sold: 0} for i in locset}\n",
    "    demand_penalty_dict = {i: {com1_sold: demand_penalty} if i == loc5 else {com1_sold: 0} for i in locset}\n",
    "\n",
    "    scenario = Scenario(name='scenario', scales=scales, scheduling_scale_level=1, network_scale_level=0,\n",
    "                        purchase_scale_level=1, availability_scale_level=1, demand_scale_level=1,\n",
    "                        capacity_scale_level=1, network=network, demand=demand_dict, demand_penalty=demand_penalty_dict,\n",
    "                        label='Stochastic scenario with Multiple Locations')\n",
    "\n",
    "    if scen_df.empty:\n",
    "        # ======================================================================================================================\n",
    "        # Declare problem\n",
    "        # ======================================================================================================================\n",
    "\n",
    "        problem_mincost = formulate(scenario=scenario,\n",
    "                                    constraints={Constraints.COST, Constraints.TRANSPORT, Constraints.RESOURCE_BALANCE,\n",
    "                                                 Constraints.INVENTORY, Constraints.PRODUCTION, Constraints.DEMAND,\n",
    "                                                 Constraints.NETWORK},\n",
    "                                    demand_sign='eq', objective=Objective.COST_W_DEMAND_PENALTY)\n",
    "\n",
    "        scale_iter = scale_tuple(instance=problem_mincost, scale_levels=scenario.network_scale_level + 1)\n",
    "        # capex_process = sum(problem_mincost.Capex_network[scale_] for scale_ in scale_iter)\n",
    "        # cost_trans_capex = sum(problem_mincost.Capex_transport_network[scale_] for scale_ in scale_iter)\n",
    "        \n",
    "        problem_mincost.first_stage_cost = Var(within=NonNegativeReals, doc='First Stage Cost')\n",
    "        \n",
    "        def first_stage_cost_rule(instance):\n",
    "            return (instance.first_stage_cost == sum(instance.Capex_network[scale_] for scale_ in scale_iter) + \n",
    "                                                sum(instance.Capex_transport_network[scale_] for scale_ in scale_iter))\n",
    "        \n",
    "        problem_mincost.constraint_first_stage_cost = Constraint(rule=first_stage_cost_rule)\n",
    "\n",
    "        return scenario, problem_mincost\n",
    "\n",
    "    else:\n",
    "        return scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e01d3fcab970c9e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_smodel(scen_df=pandas.DataFrame()):\n",
    "\n",
    "    scenario = build_model(scen_df)\n",
    "    # ======================================================================================================================\n",
    "    # Declare problem\n",
    "    # ======================================================================================================================\n",
    "\n",
    "    problem_mincost = formulate(scenario=scenario,\n",
    "                                constraints={Constraints.COST, Constraints.TRANSPORT, Constraints.RESOURCE_BALANCE,\n",
    "                                             Constraints.INVENTORY, Constraints.PRODUCTION, Constraints.DEMAND,\n",
    "                                             Constraints.NETWORK},\n",
    "                                demand_sign='eq', objective=Objective.COST_W_DEMAND_PENALTY)\n",
    "\n",
    "    scale_iter = scale_tuple(instance=problem_mincost, scale_levels=scenario.network_scale_level + 1)\n",
    "    # capex_process = sum(problem_mincost.Capex_network[scale_] for scale_ in scale_iter)\n",
    "    # cost_trans_capex = sum(problem_mincost.Capex_transport_network[scale_] for scale_ in scale_iter)\n",
    "    \n",
    "    problem_mincost.first_stage_cost = Var(within=NonNegativeReals, doc='First Stage Cost')\n",
    "    \n",
    "    def first_stage_cost_rule(instance):\n",
    "        return (instance.first_stage_cost == sum(instance.Capex_network[scale_] for scale_ in scale_iter) + \n",
    "                                            sum(instance.Capex_transport_network[scale_] for scale_ in scale_iter))\n",
    "    \n",
    "    problem_mincost.constraint_first_stage_cost = Constraint(rule=first_stage_cost_rule)\n",
    "\n",
    "    return scenario, problem_mincost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7fe86a4beed2e4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scenario_creator(scen_name, **kwargs):\n",
    "    scen_dict = kwargs.get('scenario_dict')\n",
    "    scen, model = build_smodel(scen_df=scen_dict[scen_name]['factor'])\n",
    "    sputils.attach_root_node(model, model.first_stage_cost,\n",
    "                             [model.X_P, model.Cap_P, model.X_S, model.Cap_S, model.X_F, model.Cap_F])\n",
    "    model._mpisppy_probability = scen_dict[scen_name]['prob']\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90560a18bd9eec3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PI_pickle_dump_folder = r'C:\\Users\\shivam.vedant\\PycharmProjects\\energiapy\\docs\\notebooks\\supply chain\\AIChE 2024\\Pickle Dump New'\n",
    "FD_pickle_dump_folder = r'C:\\Users\\shivam.vedant\\PycharmProjects\\energiapy\\docs\\notebooks\\supply chain\\AIChE 2024\\FD Pickle Dump New'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4572f5648606cbfd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solver_options = {\n",
    "    'MIPGap': 0.005,\n",
    "    # 'TimeLimit': 60 * 15,\n",
    "    'Heuristics': 0.20\n",
    "}\n",
    "\n",
    "event_dict = create_event_dict(_exec_scenarios)\n",
    "# scenarios_to_remove = [('res1', 'res6'), ('res1', 'cap4'), ('cap2', 'cap4'), ('trans12', 'cap4'), ('trans25', 'cap4')]\n",
    "scenario_dict = create_scenario_dict(event_dict=event_dict)\n",
    "scenario_names = list(scenario_dict.keys())\n",
    "\n",
    "with open('scenario_dict_new.pkl', 'wb') as file:\n",
    "    pickle.dump(scenario_dict, file)\n",
    "\n",
    "print(f\"Sum of probabilities of all scenarios: {sum(scenario_dict[scen]['prob'] for scen in scenario_dict):.6f}\")\n",
    "print(f'Number of considered scenarios: {len(scenario_names)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0780725920be00c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('scenario_dict_new.pkl', 'rb') as file:\n",
    "    load_scenario_dict = pickle.load(file)\n",
    "\n",
    "load_scenario_names = list(load_scenario_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce999fdec6e3a50",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exCost_PI = 0\n",
    "results_PI = dict()\n",
    "scen_PI, model_PI = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f91a5c8fe3811a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Deterministic Scenarios for Perfect Information\n",
    "counter = 0\n",
    "PI_output_dict=dict()\n",
    "\n",
    "for scen_name in load_scenario_names:\n",
    "    scen_PI = build_model(scen_df=load_scenario_dict[scen_name]['factor'])\n",
    "    counter+=1\n",
    "    # Delete process capacity factors, resource availability factors, transport capacity factors\n",
    "    model_PI.del_component('constraint_nameplate_production_varying_capacity')\n",
    "    model_PI.del_component('constraint_resource_consumption_varying')\n",
    "    model_PI.del_component('constraint_export')\n",
    "\n",
    "    # Add the constraints back for this particular scenario\n",
    "    model_PI.constraint_nameplate_production_varying_capacity = make_constraint(instance=model_PI,\n",
    "        type_cons=Cons.X_LEQ_BY, variable_x='P', location_set=model_PI.locations, component_set=model_PI.processes_varying_capacity,\n",
    "        loc_comp_dict=scen_PI.location_process_dict, b_factor=scen_PI.capacity_factor,\n",
    "                                                                                x_scale_level=scen_PI.scheduling_scale_level,\n",
    "                                                                                b_scale_level=scen_PI.capacity_scale_level,\n",
    "                                                                                y_scale_level=scen_PI.network_scale_level,\n",
    "                                                                                variable_y='Cap_P',\n",
    "                                                                                label='restricts production to varying nameplate capacity')\n",
    "\n",
    "    model_PI.constraint_resource_consumption_varying = make_constraint(\n",
    "        instance=model_PI, type_cons=Cons.X_LEQ_B, variable_x='C', location_set=model_PI.locations,\n",
    "        component_set=model_PI.resources_varying_availability, b_max=scen_PI.cons_max,\n",
    "        loc_comp_dict=scen_PI.location_resource_dict, b_factor=scen_PI.availability_factor,\n",
    "        x_scale_level=scen_PI.scheduling_scale_level, b_scale_level=scen_PI.availability_scale_level,\n",
    "        label='restricts resource consumption to varying availablity')\n",
    "\n",
    "    constraint_export(instance=model_PI, scheduling_scale_level=scen_PI.scheduling_scale_level,\n",
    "                      network_scale_level=scen_PI.network_scale_level,\n",
    "                      location_transport_resource_dict=scen_PI.location_transport_resource_dict,\n",
    "                      transport_capacity_factor=scen_PI.transport_capacity_factor,\n",
    "                      transport_capacity_scale_level=scen_PI.transport_capacity_scale_level)\n",
    "\n",
    "    results_PI = solve(scenario=scen_PI, instance=model_PI, solver='gurobi', name=scen_name,\n",
    "                       solver_options=solver_options)\n",
    "\n",
    "    print(f'######################## Finished solving {scen_name} ({counter} of {len(load_scenario_dict)}) ########################')\n",
    "\n",
    "    model_vars = model_PI.component_map(ctype=Var)\n",
    "    vars_dict = {i: model_vars[i].extract_values() for i in model_vars.keys()}\n",
    "    obj_dict = {'objective': pyoval(model_PI.objective_cost_w_demand_penalty)}\n",
    "\n",
    "    PI_output_dict[scen_name] = {**vars_dict, **obj_dict}\n",
    "\n",
    "    with open(os.path.join(PI_pickle_dump_folder, scen_name+'.pkl'), 'wb') as file:\n",
    "        pickle.dump(PI_output_dict[scen_name], file)\n",
    "\n",
    "    exCost_PI += pyoval(model_PI.objective_cost_w_demand_penalty) * load_scenario_dict[scen_name]['prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46eb97d5555dd56",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PI_load_dict = dict()\n",
    "for file in os.listdir(PI_pickle_dump_folder):\n",
    "    if file.endswith('.pkl'):\n",
    "        full_file_path = os.path.join(PI_pickle_dump_folder, file)\n",
    "        \n",
    "        with open(full_file_path, 'rb') as f:\n",
    "            PI_load_dict[file.removesuffix('.pkl')] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f291f451abf92560",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(f\"Total Expected Cost considering perfect information: {exCost_PI:.4f}\")\n",
    "\n",
    "with open('exCost_PI_new.pkl', 'wb') as file:\n",
    "    pickle.dump(exCost_PI, file)\n",
    "    \n",
    "with open('exCost_PI_new.pkl','rb') as file:\n",
    "    load_exCost_PI = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9769ddf8f89e786f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_common_substring(lst):\n",
    "    \n",
    "    # Split each string into substrings and create sets\n",
    "    substring_sets = [set(item.split()) for item in lst]\n",
    "    \n",
    "    # Find the intersection of all sets to get common substrings\n",
    "    common_substrings = reduce(lambda a, b: a & b, substring_sets)\n",
    "    \n",
    "    return common_substrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4fbc2a87af2e7e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def explore_dict(d, level=0):\n",
    "    indent = \"    \" * level  # Indentation for better readability\n",
    "    for key, value in d.items():\n",
    "        print(f\"{indent}Key: {key}, Type of value: {type(value)}\")\n",
    "        if isinstance(value, dict):  # Recursively explore dictionaries\n",
    "            explore_dict(value, level + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d764097984cc61af",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_scenarios():\n",
    "    # Initialize the result dictionary\n",
    "    result = {}\n",
    "    \n",
    "    # Step 1: Iterate over the original data\n",
    "    for scenario, data in PI_load_dict.items():\n",
    "        x_val = data['Demand_penalty_network'][('com1_sold',0)]\n",
    "        obj_val = data['objective'] - data['Demand_penalty_network'][('com1_sold',0)]\n",
    "        \n",
    "        # Step 2: Check if x_val is already a key in the result\n",
    "        if x_val not in result:\n",
    "            result[x_val] = {'count': 0, 'objectives': {}}\n",
    "        \n",
    "        # Increment the count for the x value\n",
    "        result[x_val]['count'] += 1\n",
    "        \n",
    "        # Step 3: Check if obj_val is already a key under the 'objectives' for this x\n",
    "        if obj_val not in result[x_val]['objectives']:\n",
    "            result[x_val]['objectives'][obj_val] = {'scenarios': [], 'count': 0}\n",
    "        \n",
    "        # Add the scenario to the list and increment the count for the objective\n",
    "        result[x_val]['objectives'][obj_val]['scenarios'].append(scenario)\n",
    "        result[x_val]['objectives'][obj_val]['count'] += 1\n",
    "    \n",
    "    # Step 4: Sort the result by x values (in decreasing order) and objectives (in decreasing order)\n",
    "    sorted_result = {\n",
    "        x_val: {\n",
    "            'count': result[x_val]['count'],\n",
    "            'count_obj': len(result[x_val]['objectives']),\n",
    "            'objectives': {\n",
    "                obj_val: result[x_val]['objectives'][obj_val]\n",
    "                for obj_val in sorted(result[x_val]['objectives'].keys(), reverse=True)\n",
    "            }\n",
    "        }\n",
    "        for x_val in sorted(result.keys(), reverse=True)\n",
    "    }\n",
    "    \n",
    "    return sorted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84762f470a262e68",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pick_scenarios(sorted_dict, n):\n",
    "    selected_scenarios = {}\n",
    "    total_selected = 0\n",
    "\n",
    "    # Iterate over sorted x values\n",
    "    for x_val, x_data in sorted_dict.items():\n",
    "        if total_selected >= n:\n",
    "            break  # Stop if we have selected enough scenarios\n",
    "\n",
    "        # Iterate over sorted objective values within each x\n",
    "        for obj_val, obj_data in x_data['objectives'].items():\n",
    "            if total_selected >= n:\n",
    "                break  # Stop if we have selected enough scenarios\n",
    "\n",
    "            # Randomly select from the list of scenarios if there are multiple\n",
    "            scenarios_to_choose = obj_data['scenarios']\n",
    "            num_to_pick = min(len(scenarios_to_choose), 1)\n",
    "            chosen_scenarios = random.sample(scenarios_to_choose, num_to_pick)\n",
    "\n",
    "            # Add the selected scenarios to the result\n",
    "            if x_val not in selected_scenarios:\n",
    "                selected_scenarios[x_val] = {}\n",
    "            \n",
    "            if obj_val not in selected_scenarios[x_val]:\n",
    "                selected_scenarios[x_val][obj_val] = {}\n",
    "\n",
    "            for scenario in chosen_scenarios:\n",
    "                selected_scenarios[x_val][obj_val][scenario] = {\n",
    "                    'prob': load_scenario_dict[scenario]['prob'],\n",
    "                    'factor': load_scenario_dict[scenario]['factor'].copy()\n",
    "                }\n",
    "                total_selected += 1\n",
    "\n",
    "                if total_selected >= n:\n",
    "                    break  # Stop if we have selected enough scenarios\n",
    "\n",
    "    return selected_scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f093a9b8604c924c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sum_probabilities(d):\n",
    "    total_prob = 0\n",
    "    \n",
    "    def recursive_sum(current_dict):\n",
    "        nonlocal total_prob\n",
    "        for key, value in current_dict.items():\n",
    "            if isinstance(value, dict):\n",
    "                # Recursively traverse if it's still a dictionary\n",
    "                recursive_sum(value)\n",
    "            elif key == 'prob':\n",
    "                # Add the probability value\n",
    "                total_prob += value\n",
    "\n",
    "    recursive_sum(d)\n",
    "    return total_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27d2d5c725e208",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_result = filter_scenarios()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803f58aeb9dbc6bf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(data['count_obj'] for key, data in sorted_result.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6097c1c866d135",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example data\n",
    "n = len(list(sorted_result.keys()))\n",
    "list1 = [i for i in range(1,n+1)]  # X-axis labels\n",
    "list2 = [sorted_result[key]['count'] for key, data in sorted_result.items()]\n",
    "\n",
    "# Create a bar chart\n",
    "plt.bar(list1, list2)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Demand Penalty Values')\n",
    "plt.ylabel('Number of Scenarios')\n",
    "plt.title('Demand Penalty Value Histogram')\n",
    "\n",
    "for i, value in enumerate(list2):\n",
    "    plt.text(i+1, value, str(value), ha='center', va='bottom')\n",
    "\n",
    "# Rotate the x-axis labels if they are too long or overlapping\n",
    "plt.xticks()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3444d2be3d0b2e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example data\n",
    "\n",
    "penalty_key = list(sorted_result.keys())[0]\n",
    "n = len(list(sorted_result[penalty_key]['objectives'].keys()))\n",
    "obj_keys = [list(sorted_result[penalty_key]['objectives'].keys())[i] for i in range(n)]\n",
    "list1 = [i for i in range(1,n+1)]  # X-axis labels\n",
    "list2 = [sorted_result[penalty_key]['objectives'][obj_keys[i]]['count'] for i in range(n)]\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "# Create a bar chart\n",
    "plt.bar(list1, list2, width = 0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Scenarios')\n",
    "plt.ylabel('#')\n",
    "plt.title('Number of occurences')\n",
    "\n",
    "for i, value in enumerate(list2):\n",
    "    plt.text(i+1, value, str(value), ha='center', va='bottom', fontsize=13)\n",
    "\n",
    "# Rotate the x-axis labels if they are too long or overlapping\n",
    "# plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc33e63c4a5958",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selected_scenario_names = load_scenario_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9615a1667312bc80",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "options = {\"solver\": \"gurobi\"}\n",
    "scenario_creator_kwargs = {'scenario_dict': load_scenario_dict}\n",
    "ef_UI = ExtensiveForm(options, selected_scenario_names, scenario_creator, scenario_creator_kwargs=scenario_creator_kwargs)\n",
    "results = ef_UI.solve_extensive_form(solver_options=solver_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e1107fa20c3526",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exCost_UI = ef_UI.get_objective_value()\n",
    "# exCost_UI\n",
    "\n",
    "with open(fr'{FD_pickle_dump_folder}\\{len(selected_scenario_names)}\\exCost_{len(selected_scenario_names)}_UI.pkl', 'wb') as file:\n",
    "    pickle.dump(exCost_UI, file)\n",
    "    \n",
    "with open(fr'{FD_pickle_dump_folder}\\{len(selected_scenario_names)}\\exCost_{len(selected_scenario_names)}_UI.pkl', 'wb') as file:\n",
    "    load_exCost_UI = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3dac7a901ac5db",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# exCost_UI = 2133007.7662\n",
    "EVPI = load_exCost_UI - load_exCost_PI\n",
    "p_inc = EVPI * 100 / load_exCost_PI\n",
    "p_dec = EVPI * 100 / exCost_UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c531f16dfb3c7ccf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(f\"Total Expected Cost considering perfect information: {load_exCost_PI:.4f}\")\n",
    "print(f\"Total Expected Cost considering disruptions (stochastic solution): {load_exCost_UI:.4f}\")\n",
    "print(f\"Expected Value of Perfect Information: {EVPI:.4f}\")\n",
    "print(f\"Percentage Increase in Cost: {p_inc:.4f}%%\")\n",
    "print(f\"Percentage Decrease in Cost: {p_dec:.4f}%%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c6d856eac5d3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ssoln_dict = ef_UI.get_root_solution()\n",
    "\n",
    "with open('ssoln_new.pkl', 'wb') as file:\n",
    "    pickle.dump(ssoln_dict, file)\n",
    "    \n",
    "with open('ssoln_new.pkl', 'rb') as file:\n",
    "    load_ssoln_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af20b46fc88782",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# FIXED DESIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a5c4792787fa88",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exCost_FD = 0\n",
    "exDF_FD = 0\n",
    "results_FD = dict()\n",
    "scen_FD, model_FD = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a2ce391ae3b170",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fix_variables(model1: ConcreteModel, model2:ConcreteModel, scen_name:str):\n",
    "    # vars_to_fix = ['X_P', 'Cap_P', 'X_S', 'Cap_S', 'X_F', 'Cap_F']\n",
    "    model = getattr(model1, scen_name)\n",
    "    def fix(var1, var2):\n",
    "        for i in list(var1.keys()):\n",
    "            if var1[i].value is None:\n",
    "                continue\n",
    "            else:\n",
    "                var2[i].fixed = True\n",
    "                var2[i] = var1[i].value\n",
    "            # var2[i].pprint()\n",
    "            \n",
    "    fix(model.X_P, model2.X_P)\n",
    "    fix(model.Cap_P, model2.Cap_P)\n",
    "    fix(model.X_S, model2.X_S)\n",
    "    fix(model.Cap_S, model2.Cap_S)\n",
    "    fix(model.X_F, model2.X_F)\n",
    "    fix(model.Cap_F, model2.Cap_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce6fb5fdf849b3a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Deterministic Scenarios for Fixed Design\n",
    "counter = 0\n",
    "FD_output_dict=dict()\n",
    "fix_variables(model1=ef_UI.ef, model2=model_FD, scen_name=selected_scenario_names[0])\n",
    "\n",
    "for scen_name in selected_scenario_names:\n",
    "    scen_FD = build_model(scen_df=load_scenario_dict[scen_name]['factor'])\n",
    "    counter+=1\n",
    "    # Delete process capacity factors, resource availability factors, transport capacity factors\n",
    "    model_FD.del_component('constraint_nameplate_production_varying_capacity')\n",
    "    model_FD.del_component('constraint_resource_consumption_varying')\n",
    "    model_FD.del_component('constraint_export')\n",
    "\n",
    "    # Add the constraints back for this particular scenario\n",
    "    model_FD.constraint_nameplate_production_varying_capacity = make_constraint(instance=model_FD,\n",
    "        type_cons=Cons.X_LEQ_BY, variable_x='P', location_set=model_FD.locations, component_set=model_FD.processes_varying_capacity,\n",
    "        loc_comp_dict=scen_FD.location_process_dict, b_factor=scen_FD.capacity_factor,\n",
    "                                                                                x_scale_level=scen_FD.scheduling_scale_level,\n",
    "                                                                                b_scale_level=scen_FD.capacity_scale_level,\n",
    "                                                                                y_scale_level=scen_FD.network_scale_level,\n",
    "                                                                                variable_y='Cap_P',\n",
    "                                                                                label='restricts production to varying nameplate capacity')\n",
    "\n",
    "    model_FD.constraint_resource_consumption_varying = make_constraint(\n",
    "        instance=model_FD, type_cons=Cons.X_LEQ_B, variable_x='C', location_set=model_FD.locations,\n",
    "        component_set=model_FD.resources_varying_availability, b_max=scen_FD.cons_max,\n",
    "        loc_comp_dict=scen_FD.location_resource_dict, b_factor=scen_FD.availability_factor,\n",
    "        x_scale_level=scen_FD.scheduling_scale_level, b_scale_level=scen_FD.availability_scale_level,\n",
    "        label='restricts resource consumption to varying availablity')\n",
    "\n",
    "    constraint_export(instance=model_FD, scheduling_scale_level=scen_FD.scheduling_scale_level,\n",
    "                      network_scale_level=scen_FD.network_scale_level,\n",
    "                      location_transport_resource_dict=scen_FD.location_transport_resource_dict,\n",
    "                      transport_capacity_factor=scen_FD.transport_capacity_factor,\n",
    "                      transport_capacity_scale_level=scen_FD.transport_capacity_scale_level)\n",
    "\n",
    "    results_FD = solve(scenario=scen_FD, instance=model_FD, solver='gurobi', name=scen_name,\n",
    "                       solver_options=solver_options)\n",
    "\n",
    "    print(f'######################## Finished solving {scen_name} ({counter} of {len(load_scenario_dict)}) ########################')\n",
    "\n",
    "    model_vars = model_FD.component_map(ctype=Var)\n",
    "    vars_dict = {i: model_vars[i].extract_values() for i in model_vars.keys()}\n",
    "    obj_dict = {'objective': pyoval(model_FD.objective_cost_w_demand_penalty)}\n",
    "\n",
    "    FD_output_dict[scen_name] = {**vars_dict, **obj_dict}\n",
    "\n",
    "    folder_path = r'C:\\Users\\shivam.vedant\\PycharmProjects\\energiapy\\docs\\notebooks\\supply chain\\AIChE 2024\\Ext FD Pickle Dump'\n",
    "    \n",
    "    with open(os.path.join(folder_path, str(len(selected_scenario_names)), scen_name+'.pkl'), 'wb') as file:\n",
    "        pickle.dump(FD_output_dict[scen_name], file)\n",
    "\n",
    "    exCost_FD += pyoval(model_FD.objective_cost_w_demand_penalty) * load_scenario_dict[scen_name]['prob']\n",
    "    exDF_FD += pyoval(model_FD.Demand_penalty_network[('com1_sold',0)]) * load_scenario_dict[scen_name]['prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb77824b2b58fc1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_results_dict = {len(selected_scenario_names): {'Expected Cost UI': exCost_UI,\n",
    "                                            'Expected Cost FD': exCost_FD,\n",
    "                                            'Expected Resilience': 1- exDF_FD/(20*400*90*4)}}\n",
    "    \n",
    "with open(os.path.join(folder_path, str(len(selected_scenario_names)),'FD_final_results.pkl'), 'wb') as file:\n",
    "    pickle.dump(final_results_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6644572aadab8a2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffda6c3dc771e0c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
